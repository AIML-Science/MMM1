{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This module contains all functions relating to the cleaning, exploration and feature engineering of structured data sets; mostly in pandas format\n",
    "\n",
    "\n",
    "install the anaconda python distribution to be able to use numpy, pandas, matplotlib, seaborn, scipy, sklearn and datetime\n",
    "\n",
    "use pip install category_encoders for category_encoders \n",
    "use pip install category_embedder for category_embedder. It requires the tensorflow and keras\n",
    "'''\n",
    "\n",
    "##pip install category_encoders\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from category_encoders import *\n",
    "from IPython.display import display\n",
    "from collections import Counter\n",
    "import scipy.stats as sp\n",
    "import datetime as dt\n",
    "import re\n",
    "#import categorical_embedder\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, RobustScaler, StandardScaler\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory and Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cat_feats(data=None):\n",
    "    '''\n",
    "    Returns the categorical features in a data set\n",
    "    Parameters:\n",
    "    -----------\n",
    "        data: DataFrame or named Series \n",
    "    Returns:\n",
    "    -------\n",
    "        List\n",
    "            A list of all the categorical features in a dataset.\n",
    "    it is used as a helper function for most of the functions to get categorical variables\n",
    "    '''\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame or Series, got 'None'\")\n",
    "\n",
    "    cat_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "    return list(cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_feats(data=None):\n",
    "    '''\n",
    "    Returns the numerical features in a data set\n",
    "    Parameters:\n",
    "    -----------\n",
    "        data: DataFrame or named Series \n",
    "    Returns:\n",
    "    -------\n",
    "        List:\n",
    "            A list of all the numerical features in a dataset.\n",
    "    it is used as a helper function for most of the functions to get categorical variables\n",
    "    '''\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame or Series, got 'None'\")\n",
    "\n",
    "    num_features = data.select_dtypes(include=numerics).columns\n",
    "\n",
    "    return list(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_counts(data=None):\n",
    "    '''\n",
    "    Gets the unique count of categorical features in a data set.\n",
    "    Parameters\n",
    "    -----------\n",
    "        data: DataFrame or named Series \n",
    "    Returns\n",
    "    -------\n",
    "        DataFrame or Series\n",
    "            Unique value counts of the features in a dataset.\n",
    "    it is used as a helper function in the describe function to get the count of unique values in the columns \n",
    "    '''\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame or Series, got 'None'\")\n",
    "\n",
    "    features = get_cat_feats(data)\n",
    "    temp_len = []\n",
    "\n",
    "    for feature in features:\n",
    "        temp_len.append(len(data[feature].unique()))\n",
    "        \n",
    "    df = list(zip(features, temp_len))\n",
    "    df = pd.DataFrame(df, columns=['Feature', 'Unique Count'])\n",
    "    df = df.style.bar(subset=['Unique Count'], align='mid')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_missing(data=None, plot=False):\n",
    "    '''\n",
    "    Display missing values as a pandas dataframe.\n",
    "    Parameters\n",
    "    ----------\n",
    "        data: DataFrame or named Series\n",
    "        plot: bool, Default False\n",
    "            Plots missing values in dataset as a heatmap\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        Matplotlib Figure:\n",
    "            Heatmap plot of missing values\n",
    "    '''\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame or Series, got 'None'\")\n",
    "\n",
    "    df = data.isna().sum()\n",
    "    df = df.reset_index()\n",
    "    df.columns = ['features', 'missing_counts']\n",
    "\n",
    "    missing_percent = round((df['missing_counts'] / data.shape[0]) * 100, 1)\n",
    "    df['missing_percent'] = missing_percent\n",
    "\n",
    "    if plot:\n",
    "        plot_missing(data)\n",
    "        return df\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_summarizer(data, x=None, y=None, hue=None, palette='Set1', verbose=True):\n",
    "    '''\n",
    "    Helper function that gives a quick summary of a given column of categorical data\n",
    "    Parameters:\n",
    "    ---------------------------\n",
    "        dataframe: pandas dataframe\n",
    "        x: str.\n",
    "            horizontal axis to plot the labels of categorical data, y would be the count.\n",
    "        y: str. \n",
    "            vertical axis to plot the labels of categorical data, x would be the count.\n",
    "        hue: str. i\n",
    "            if you want to compare it another variable (usually the target variable)\n",
    "        palette: array, list.\n",
    "            Colour of the plot\n",
    "    Returns:\n",
    "    ----------------------\n",
    "        Quick Stats of the data and also the count plot\n",
    "        \n",
    "        it is used in the describe function\n",
    "    '''\n",
    "    if x == None:\n",
    "        column_interested = y\n",
    "    else:\n",
    "        column_interested = x\n",
    "    series = data[column_interested]\n",
    "    print(series.describe())\n",
    "    print('mode: ', series.mode())\n",
    "    if verbose:\n",
    "        print('='*80)\n",
    "        print(series.value_counts())\n",
    "\n",
    "    sns.countplot(x=x, y=y, hue=hue, data=data, palette=palette)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _space():\n",
    "    '''it is used in  most functions to add space. this makes result more presentation'''\n",
    "    print('\\n')\n",
    "def _match_date(data):\n",
    "    '''\n",
    "        Return a list of columns that matches the DateTime expression\n",
    "    '''\n",
    "    mask = data.sample(20).astype(str).apply(lambda x : x.str.match(r'(\\d{2,4}-\\d{2}-\\d{2,4})+').all())\n",
    "    return set(data.loc[:, mask].columns)\n",
    "\n",
    "\n",
    "def display_rows(data,num=2):\n",
    "    '''\n",
    "    Displays the required number of rows\n",
    "    it is used in the describe function\n",
    "    '''\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame/ numpy2d array, got 'None'\")\n",
    "\n",
    "    return data.head(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_missing(data=None):\n",
    "    '''\n",
    "    Plots the data as a heatmap to show missing values\n",
    "    Parameters\n",
    "    ----------\n",
    "        data: DataFrame, array, or list of arrays.\n",
    "            The data to plot.\n",
    "    '''\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame or Series, got 'None'\")\n",
    "\n",
    "    sns.heatmap(data.isnull(), cbar=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_count(data=None, features=None, plot=False, save_fig=False):\n",
    "    '''\n",
    "    Displays the number of classes in a categorical feature.\n",
    "    Parameters:\n",
    "    \n",
    "        data: Pandas DataFrame or Series\n",
    "            Dataset for plotting.\n",
    "        features: Scalar, array, or list. \n",
    "            The categorical features in the dataset, if None, \n",
    "            we try to infer the categorical columns from the dataframe.\n",
    "        plot: bool, Default False.\n",
    "            Plots the class counts as a barplot\n",
    "        save_fig: bool, Default False.\n",
    "            Saves the plot to the current working directory.\n",
    "    it is used in the describe function\n",
    "    '''\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame or Series, got 'None'\")\n",
    "\n",
    "    if features is None:\n",
    "        features = get_cat_feats(data)\n",
    "\n",
    "                        \n",
    "\n",
    "    for feature in features:\n",
    "        if data[feature].nunique() > 15:\n",
    "            print(\"Unique classes in {} too large\".format(feature))\n",
    "        else:\n",
    "            print('Class Count for', feature)\n",
    "            display(pd.DataFrame(data[feature].value_counts()))\n",
    "\n",
    "    if plot:\n",
    "        countplot(data, features, save_fig=save_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_cols(data=None):\n",
    "    '''\n",
    "    Returns the Datetime columns in a data set.\n",
    "    Parameters\n",
    "    ----------\n",
    "        data: DataFrame or named Series\n",
    "            Data set to infer datetime columns from.\n",
    "        convert: bool, Default True\n",
    "            Converts the inferred date columns to pandas DateTime type\n",
    "    Returns:\n",
    "    -------\n",
    "        List\n",
    "         Date column names in the data set\n",
    "    use in the describe function to set date columns to datetime datatype in utc\n",
    "    '''\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame or Series, got 'None'\")\n",
    "\n",
    "    #Get existing date columns in pandas Datetime64 format\n",
    "    date_cols = set(data.dtypes[data.dtypes == 'datetime64[ns, UTC]'].index)\n",
    "    #infer Date columns \n",
    "    date_cols = date_cols.union(_match_date(data))\n",
    "       \n",
    "    return date_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bivariate_stats(data):\n",
    "    '''Returns the contingency table and chi2 contingency test result between columns in the dataframe\n",
    "        \n",
    "        it is used in the describe function for categorical features analysis \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    cat_feats = get_cat_feats(data=data)\n",
    "    counter = 1\n",
    "    try:\n",
    "        while counter<(len(cat_feats)):\n",
    "            val1 = get_cat_feats(data=datainput)[counter - 1]\n",
    "            val2 = get_cat_feats(data=datainput)[counter]\n",
    "            if (data[val1].nunique() > 15) or (data[val2].nunique() > 15):\n",
    "                print('Number of unique values too large')\n",
    "            else:\n",
    "                freqtab = pd.crosstab(data[val1], data[val2])\n",
    "                print(\"Frequency table\")\n",
    "                print(\"============================\")\n",
    "                print(freqtab)\n",
    "                print(\"============================\")\n",
    "                chi2, pval, dof, expected = sp.chi2_contingency(freqtab)\n",
    "                print(\"ChiSquare test statistic: \",chi2)\n",
    "                print(\"p-value: \",pval)\n",
    "                _space()\n",
    "            counter= counter+1\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bivariate_stats_target(data, target):\n",
    "    \n",
    "    '''Returns the contingency table and chi2 contingency test result between columns and the target variable in the dataframe\n",
    "        \n",
    "        \n",
    "        Parameters\n",
    "    ----------\n",
    "        data: DataFrame or named Series\n",
    "            Data set to infer datetime columns from.\n",
    "        target: the target variable in form of string\n",
    "\n",
    "        \n",
    "        it is used in the describe function for categorical features analysis of the relationship between the target variable \n",
    "        and other categorical features\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    cat_feats = get_cat_feats(data=data)\n",
    "    for i in cat_feats:\n",
    "        if (data[i].nunique() > 20):\n",
    "            print('Number of Unique values too large')\n",
    "        else:\n",
    "            freqtab = pd.crosstab(data[i], data[target])\n",
    "            print(\"Frequency table\")\n",
    "            print(\"============================\")\n",
    "            print(freqtab)\n",
    "            print(\"============================\")\n",
    "            chi2, pval, dof, expected = sp.chi2_contingency(freqtab)\n",
    "            print(\"ChiSquare test statistic: \",chi2)\n",
    "            print(\"p-value: \",pval)\n",
    "            _space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Processing; Missing Value Imputation, Date Feature Extraction and Categorical Feature Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-293c67612618>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-13-293c67612618>\"\u001b[1;36m, line \u001b[1;32m14\u001b[0m\n\u001b[1;33m    data[f'month_{i}'] = data[i].dt.month\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def date_processing(data=None, date_cols = None, utc=True):\n",
    "    '''\n",
    "    This function is used for preprocessing date columns and creating new columns from the date columns.\n",
    "    \n",
    "    It is used in the feature_processing function.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame/ numpy2d array, got 'None'\")\n",
    "        \n",
    "    for i in date_cols:\n",
    "        data[i] = pd.to_datetime(data[i], utc=utc)\n",
    "        data[f'month_{i}'] = data[i].dt.month\n",
    "        data[f'year_{i}'] = data[i].dt.year\n",
    "        data[f'day_{i}'] = data[i].dt.day\n",
    "        data[f'dayweek_{i}'] = data[i].dt.weekday\n",
    "        #data = data.drop(date_cols, axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing(data=None, percent=99):\n",
    "    '''\n",
    "    Drops missing columns with [percent] of missing data.\n",
    "    Parameters:\n",
    "    -------------------------\n",
    "        data: Pandas DataFrame or Series.\n",
    "        percent: float, Default 99\n",
    "            Percentage of missing values to be in a column before it is eligible for removal.\n",
    "    Returns:\n",
    "    ------------------\n",
    "        Pandas DataFrame or Series.\n",
    "    It can be used alone. It also used in deal_with_missing_value function.\n",
    "    \n",
    "    This function is used in the deal_with_missing_value function.\n",
    "    '''\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame/ numpy2d array, got 'None'\")\n",
    "    \n",
    "    missing_percent = (data.isna().sum() / data.shape[0]) * 100\n",
    "    cols_2_drop = missing_percent[missing_percent.values >= percent].index\n",
    "    print(\"Dropped {}\".format(list(cols_2_drop)))\n",
    "    #Drop missing values\n",
    "    df = data.drop(cols_2_drop, axis=1)\n",
    "    return df\n",
    "\n",
    "def fill_missing_cats(data=None, cat_features=None, missing_encoding=None, missing_col=False):\n",
    "    '''\n",
    "    Fill missing values using the mode of the categorical features.\n",
    "    Parameters:\n",
    "    ------------------------\n",
    "        data: DataFrame or name Series.\n",
    "            Data set to perform operation on.\n",
    "        cat_features: List, Series, Array.\n",
    "            categorical features to perform operation on. If not provided, we automatically infer the categoricals from the dataset.\n",
    "        missing_encoding: List, Series, Array.\n",
    "            Values used in place of missing. Popular formats are [-1, -999, -99, '', ' ']\n",
    "        missin_col: bool, Default True\n",
    "      Creates a new column to capture the missing values. 1 if missing and 0 otherwise. This can sometimes help a machine learning model.\n",
    "      \n",
    "      This function is used in the deal_with_missing_value function.\n",
    "    '''\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame/ numpy2d array, got 'None'\")\n",
    "\n",
    "    if cat_features is None:\n",
    "        cat_features = get_cat_feats(data)\n",
    "\n",
    "    df = data.copy()\n",
    "    #change all possible missing values to NaN\n",
    "    if missing_encoding is None:\n",
    "        missing_encoding = ['', ' ', -99, -999]\n",
    "\n",
    "    df.replace(missing_encoding, np.NaN, inplace=True)\n",
    "    \n",
    "    for feat in cat_features:\n",
    "        if missing_col:\n",
    "            df[feat + '_missing_value'] = (df[feat].isna()).astype('int64')\n",
    "        most_freq = df[feat].mode()[0]\n",
    "        df[feat] = df[feat].replace(np.NaN, most_freq)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def fill_missing_num(data=None, num_features=None, method='mean', missing_col=False):\n",
    "    '''\n",
    "    fill missing values in numerical columns with specified [method] value\n",
    "    Parameters:\n",
    "        ------------------------------\n",
    "        data: DataFrame or name Series.\n",
    "            The data set to fill\n",
    "        features: list.\n",
    "            List of columns to fill\n",
    "        method: str, Default 'mean'.\n",
    "            method to use in calculating fill value.\n",
    "        missing_col: bool, Default True\n",
    "          Creates a new column to capture the missing values. 1 if missing and 0 otherwise. This can sometimes help a machine learning model.\n",
    "          \n",
    "          This function is used in the deal_with_missing_value function.\n",
    "    '''\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame/ numpy2d array, got 'None'\")\n",
    "    \n",
    "    if num_features is None:\n",
    "        num_features = get_num_feats(data)\n",
    "        #get numerical features with missing values\n",
    "        temp_df = data[num_features].isna().sum()\n",
    "        features = list(temp_df[num_features][temp_df[num_features] > 0].index)\n",
    "        \n",
    "    df = data.copy()\n",
    "    for feat in features:\n",
    "        if missing_col:\n",
    "            df[feat + '_missing_value'] = (df[feat].isna()).astype('int64')\n",
    "        if method is 'mean':\n",
    "            mean = df[feat].mean()\n",
    "            df[feat].fillna(mean, inplace=True)\n",
    "        elif method is 'median':\n",
    "            median = df[feat].median()\n",
    "            df[feat].fillna(median, inplace=True)\n",
    "        elif method is 'mode':\n",
    "            mode = df[feat].mode()[0]\n",
    "            df[feat].fillna(mode, inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"method: must specify a fill method, one of [mean, mode or median]'\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_with_missing_value(data, percent=70):\n",
    "    \"\"\"\n",
    "    this function automatically take care of missing values.\n",
    "        It fills the missing values in categorical variables with mode of the particular column\n",
    "        and fills the missing value numerical variables with mean of the particular column.\n",
    "        It automatically drops columns with more than 70% missing values except when set otherwise.\n",
    "        \n",
    "        This function is used in the feature_preprocessing function to deal with missing values. It can also be used alone.\n",
    "        \"\"\"\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame/ numpy2d array, got 'None'\")\n",
    "    df1 = drop_missing(data=data, percent=percent)\n",
    "    df2 = fill_missing_cats(data=df1)\n",
    "    df = fill_missing_num(data=df2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_redundant(data):\n",
    "    '''\n",
    "    Removes features with the same value in all cell. Drops feature If Nan is the second unique class as well.\n",
    "    Parameters:\n",
    "    -----------------------------\n",
    "        data: DataFrame or named series.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame or named series.\n",
    "    This function is used in the feature_processing function.\n",
    "    '''\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame/ numpy2d array, got 'None'\")\n",
    "    \n",
    "    #get columns\n",
    "    cols_2_drop = _nan_in_class(data)\n",
    "    print(\"Dropped {}\".format(cols_2_drop))\n",
    "    df = data.drop(cols_2_drop, axis=1)\n",
    "    return df\n",
    "def _nan_in_class(data):\n",
    "    \"\"\"helper function for drop_redundant function\"\"\"\n",
    "    cols = []\n",
    "    for col in data.columns:\n",
    "        if len(data[col].unique()) == 1:\n",
    "            cols.append(col)\n",
    "\n",
    "        if len(data[col].unique()) == 2:\n",
    "            if np.nan in list(data[col].unique()):\n",
    "                cols.append(col)\n",
    "\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Encoding Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Encoding for object to numeric conversion\n",
    "def binaryencoder(data):\n",
    "    \"\"\"To avoid the curse of dimensionality, this function only encodes categorical features with less than 7 unique values\n",
    "    \n",
    "        This function can be used alone. It is also used in the encode_data function.\n",
    "        \n",
    "        It is the default method for encoding categorical variables with unique value less than four in the encode_data function.\n",
    "        \n",
    "        explanation: It is similar to onehot encoding but gives lesser dimensions, making it a better option. \n",
    "                    It converts the unique entry into binary combination and then creates column using binary hashing.\n",
    "    \"\"\"\n",
    "    features = get_cat_feats(data=data)\n",
    "    cols = []\n",
    "    for feature in features:\n",
    "        if data[feature].nunique() < 7:\n",
    "            cols.append(feature)\n",
    "        \n",
    "    enc = BinaryEncoder(cols=cols).fit(data)\n",
    "    data = enc.transform(data)\n",
    "    return data\n",
    "\n",
    "def onehotencoder(data):\n",
    "    \"\"\"To avoid the curse of dimensionality, this function only encodes categorical features with less than 4 unique values\n",
    "    \n",
    "        This function can be used alone. It is also used in the encode_data function\n",
    "        \n",
    "    The onehotencoder is only used when the number of unique value is less than four to avoid the curse of dimensionality.\n",
    "    If encode_data method parameter is set to 'onehotencode' this is what is used in encoding categorical variables with \n",
    "    number of unique values less than 4.\n",
    "    \n",
    "    explanation: it is used mostly for nominal variables such that a binary combination of the unique values are set as new columns\n",
    "                    in the dataset.\n",
    "    \"\"\"\n",
    "    features = get_cat_feats(data=data)\n",
    "    cols = []\n",
    "    for feature in features:\n",
    "        if data[feature].nunique() < 4:\n",
    "            cols.append(feature)\n",
    "        \n",
    "    enc = OneHotEncoder(cols=cols).fit(data)\n",
    "    data = enc.transform(data)\n",
    "    return data\n",
    "\n",
    "def labelencoder(data):\n",
    "    \"\"\"\n",
    "    This function can be used alone. It is also used in the encode_data function.\n",
    "    \n",
    "    It is used for columns that has more than 3 unique values. Such columns are treated as ordinal variables. \n",
    "    \n",
    "    Explanation: Label encoders are ordinal encoders that encode unique values as continuous intergers.\n",
    "    \n",
    "    \"\"\"\n",
    "    features = get_cat_feats(data=data)\n",
    "    for feat in features:\n",
    "        data[feat] = le.fit_transform(data[feat].astype(str))\n",
    "    return data\n",
    "\n",
    "def sumencoder(data):\n",
    "    \"\"\"\n",
    "    This function can be used alone. It is also used in the encode_data function.\n",
    "    \n",
    "    The sumencoder is only used when the number of unique value is less than four to avoid the curse of dimensionality.\n",
    "    If encode_data method parameter is set to 'sumencode' this is what is used in encoding categorical variables with \n",
    "    number of unique values less than 4.\n",
    "    \n",
    "    explanation: it is similar to one-hot encoding but the difference is that in sum encoding one value is taken as '-1'\n",
    "                and it is not compared to other value.\n",
    "    \"\"\"\n",
    "\n",
    "    features = get_cat_feats(data=data)\n",
    "    cols = []\n",
    "    for feature in features:\n",
    "        if data[feature].nunique() < 4:\n",
    "            cols.append(feature)\n",
    "    enc = SumEncoder(cols = cols).fit_transform(data)\n",
    "    data = enc\n",
    "    return data\n",
    "\n",
    "def catboostencoder(data, target):\n",
    "    '''Data inputs must not be string\n",
    "        This function is used alone. It is not called by any other function.\n",
    "        \n",
    "        it uses the catboost tree model in properly encoding categorical features.\n",
    "    \n",
    "        explanation: a target encoder. It uses the target variable in encoding the categorical variables. \n",
    "        It is more accurate than most encoding methods.\n",
    "    '''\n",
    "    X = data.drop(target, axis=1)\n",
    "    y = data[target]\n",
    "    features = get_cat_feats(data=X)\n",
    "    enc = CatBoostEncoder(cols=features).fit(X,y)\n",
    "    data = enc.transform(X, y)\n",
    "    return data\n",
    "\n",
    "def hashencoder(data):\n",
    "    '''\n",
    "         This function can be used alone. It is also used in the encode_data function.\n",
    "    \n",
    "        The hashencoder is only used when the number of unique value is less than four to avoid the curse of dimensionality.\n",
    "        If encode_data method parameter is set to 'hashencode' this is what is used in encoding categorical variables with \n",
    "        number of unique values less than 4.\n",
    "\n",
    "        explanation: Feature hashing maps each category in a categorical feature to an integer within a predetermined range\n",
    "    \n",
    "                        The size of the output dimensions is controlled by the variable n_components.\n",
    "    '''    \n",
    "    \n",
    "    cols = get_cat_feats(data)\n",
    "    new_col = []\n",
    "    for i in cols:\n",
    "        string = str(data[i][0]) + str(data[i][len(data)-1]) \n",
    "        flag = re.findall(r'\\d+', string)\n",
    "        if len(flag) > 2:\n",
    "               if len(flag[0])>2:\n",
    "                    new_col.append(i)\n",
    "        \n",
    "    enc = HashingEncoder(cols=new_col, n_components= 1).fit(data)\n",
    "    data = enc.transform(data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def embeddingencoder(data):\n",
    "     \"\"\"\n",
    "        This function is used alone.\n",
    "        It uses neural network embeddings to encode categorical features. \n",
    "         \"\"\"\n",
    "     embedding_info = ce.get_embedding_info(data)\n",
    "     X_encoded,encoders = ce.get_label_encoded_data(data)\n",
    "\n",
    "     return X_endoded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(data, method='binary'):\n",
    "    \"\"\"\n",
    "        encodes categorical variables automatically using binary encoding for columns with less than 4 unique values\n",
    "        then label encode all other variables\n",
    "        method takes either binary or onehot or sumencode or hashcode. default is binary\n",
    "        \n",
    "        this function can be used alone and it also used in the feature_processing function.\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    if method == 'binary':\n",
    "        data = binaryencoder(data)\n",
    "    elif method== 'onehot':\n",
    "        data = onehotencoder(data)\n",
    "    elif method == 'sumencode':\n",
    "        data = sumencode(data)\n",
    "    data = labelencoder(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qcut(data=None, col=None, q=None, duplicates='drop', return_type='float64'):\n",
    "    '''\n",
    "    Cuts a series into bins using the pandas qcut function\n",
    "    and returns the resulting bins as a series for merging.\n",
    "    Parameter:\n",
    "    -------------\n",
    "        data: DataFrame, named Series\n",
    "            Data set to perform operation on.\n",
    "        col: str\n",
    "            column to cut/binnarize.\n",
    "        q: integer or array of quantiles\n",
    "            Number of quantiles. 10 for deciles, 4 for quartiles, etc. Alternately array of quantiles, e.g. [0, .25, .5, .75, 1.] for quartiles.\n",
    "        duplicates: Default 'drop',\n",
    "            If bin edges are not unique drop non-uniques.\n",
    "        return_type: dtype, Default (float64)\n",
    "            Dtype of series to return. One of [float64, str, int64]\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "        Series, 1D-Array\n",
    "        \n",
    "        This function is not used in any other function. It can be used alone.\n",
    "    '''\n",
    "\n",
    "    temp_df = pd.qcut(data[col], q=q, duplicates=duplicates).to_frame().astype('str')\n",
    "    #retrieve only the qcut categories\n",
    "    df = temp_df[col].str.split(',').apply(lambda x: x[0][1:]).astype(return_type)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def minmaxscaler(data, cols = None):\n",
    "    '''\n",
    "    It is scaling method use when you want all the values in column to be between 0 and 1. This is used for parametric models\n",
    "    like SVM and KNN that calculate euclidean distance or any other form of distance using the magnitude of the values.\n",
    "    \n",
    "    -------------\n",
    "        data: DataFrame, named Series\n",
    "            Data set to perform operation on. It advisable not to scale/normalise the target variable.\n",
    "        col: list of str\n",
    "            columns in form of a list to scale/normalise. If not parsed, it scales/normalises the entire dataframe\n",
    "        Returns:\n",
    "    --------\n",
    "        DataFrame of the scaled/normalised data/columns.\n",
    "    It is used in the scale_normalise_data. It can also be used alone.\n",
    "    '''\n",
    "    \n",
    "    if cols is not None:\n",
    "        mm_scaler = MinMaxScaler()\n",
    "        data[cols] = mm_scaler.fit_transform(data[cols])\n",
    "       \n",
    "    else:\n",
    "        col_names = data.columns\n",
    "        mm_scaler = MinMaxScaler()\n",
    "        df_mm = mm_scaler.fit_transform(data)\n",
    "        data = pd.DataFrame(df_mm, columns=col_names)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardscaler(data, cols = None):\n",
    "    ''' \n",
    "    It is a standard normalization technique use when using machine learning models that assusme a normal/gaussian distribution.\n",
    "    Models like Linear Regression, Gaussian Naive Bayes etc.\n",
    "    -------------\n",
    "        data: DataFrame, named Series\n",
    "            Data set to perform operation on. It advisable not to scale/normalise the target variable.\n",
    "        col: list of str\n",
    "            columns in form of a list to scale/normalise. If not parsed, it scales/normalises the entire dataframe\n",
    "        Returns:\n",
    "    --------\n",
    "        DataFrame of the scaled/normalised data/columns.\n",
    "        \n",
    "    It is used in the scale_normalise_data. It can also be used alone.\n",
    "     '''\n",
    "    if cols is not None:\n",
    "        \n",
    "        s_scaler = StandardScaler()\n",
    "        data[cols] = s_scaler.fit_transform(data[cols])\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        col_names = data.columns\n",
    "        s_scaler = StandardScaler()\n",
    "        df_s = s_scaler.fit_transform(data)\n",
    "        data = pd.DataFrame(df_s, columns=col_names)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robustscaler(data, cols = None):\n",
    "    \n",
    "    '''\n",
    "    It is similar to the standardscaler, except that is is better used when there are outliers in the dataset.\n",
    "    -------------\n",
    "        data: DataFrame, named Series\n",
    "            Data set to perform operation on. It advisable not to scale/normalise the target variable.\n",
    "        col: list of str\n",
    "            columns in form of a list to scale/normalise. If not parsed, it scales/normalises the entire dataframe\n",
    "        Returns:\n",
    "    --------\n",
    "        DataFrame of the scaled/normalised data/columns.\n",
    "    \n",
    "    It is used in the scale_normalise_data. It can also be used alone.\n",
    "        '''\n",
    "    \n",
    "    if cols is not None:\n",
    "        r_scaler = RobustScaler()\n",
    "        data[cols] = r_scaler.fit_transform(data[cols])\n",
    "    else:\n",
    "        col_names = data.columns\n",
    "        r_scaler = RobustScaler()\n",
    "        df_r = r_scaler.fit_transform(data)\n",
    "        data = pd.DataFrame(df_r, columns=col_names)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conventional_feat_scaling(data, cols = None, method = 'log', interval=[0,1]):\n",
    "    \n",
    "    \"\"\"\n",
    "    It uses conventional methodz for scaling variables.\n",
    "    -------------\n",
    "        data: DataFrame, named Series\n",
    "            Data set to perform operation on. It advisable not to scale/normalise the target variable.\n",
    "        col: list of str\n",
    "            columns in form of a list to scale/normalise. If cols is not parsed, it won't work.\n",
    "        method:str\n",
    "            the method of scaling to be used. It can be 'log','linear', 'clipping' or 'z_score'\n",
    "        interval: list of int\n",
    "            this is used by the clipping scaling method. the first element is the min val and the second element is the mex val.\n",
    "        Returns:\n",
    "    --------\n",
    "        DataFrame of the scaled/normalised data.\n",
    "    It is used alone.\n",
    "    \"\"\"\n",
    "    if cols is not None:\n",
    "        for i in cols:\n",
    "            if method == 'log':\n",
    "                data[i] = np.log((1+data[i])/2)\n",
    "            if method == 'linear':\n",
    "                maximum = np.max(data[i])\n",
    "                data[i] = data[i]/maximum\n",
    "            if method == 'clipping':\n",
    "                data[i] = np.clip(data[i], interval[0], interval[1])\n",
    "            if method == 'z_score':\n",
    "                data[i] = (data[i] - np.mean(data[i]))/np.std(data[i])\n",
    "                \n",
    "    else: \n",
    "        print(\"Input the cols parameter. This function works on the columns parsed in the cols parameter\")\n",
    "    \n",
    "    return data\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_normalize_data(data, cols = None, method = 'normalize'):\n",
    "    '''Normalises or Scale variables in the dataset. \n",
    "        NB:1. Normalise all features if the ml model assumes a normal/gaussian distribution.\n",
    "                Example of such models are linear regression, discriminant analysis (LDA) and Gaussian naive Bayes.\n",
    "                If statistical test such as t-test, ANOVA and other tests that assume normal distribution is to be carried out, \n",
    "                then normalise the data before carryingout the test.\n",
    "            2. Scale the data if to be in range of -1 and 1 \n",
    "                if parametric model like K Nearest Neighbour and Support Vector Machines are to be used.\n",
    "        \n",
    "        -------------\n",
    "        data: DataFrame, named Series\n",
    "            Data set to perform operation on. It advisable not to scale/normalise the target variable.\n",
    "        col: list of str\n",
    "            columns in form of a list to scale/normalise. If not parsed, it scales/normalises the entire dataframe\n",
    "        method: str\n",
    "            The method can either be normalize, normalize_outlier, or scale. It differes base on the used case. The default is normalise.\n",
    "        return_type: DataFrame\n",
    "            \n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "        DataFrame of the scaled/normalised data/columns.\n",
    "        \n",
    "    This function is not used in any other function. It can be used alone.\n",
    "    '''\n",
    "    data = data[get_num_feats(data=data)]\n",
    "    if method == 'normalize':\n",
    "        df = standardscaler(data, cols = cols)\n",
    "    elif method == 'normalize_outlier':\n",
    "        df = robustscaler(data, cols = cols)\n",
    "    elif method == 'scale':\n",
    "        df = minmaxscaler(data, cols = cols)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(data=None, name='', date_cols=None, show_categories=False, plot_missing=False, target = None):\n",
    "    '''\n",
    "    Calculates statistics and information about a data set. Information displayed are\n",
    "    shapes, size, number of categorical/numeric/date features, missing values,\n",
    "    dtypes of objects etc.\n",
    "    Parameters:\n",
    "    --------------------\n",
    "        data: Pandas DataFrame\n",
    "            The data to describe.\n",
    "        name: str, optional\n",
    "            The name of the data set passed to the function.\n",
    "        date_cols: list/series/array\n",
    "            Date column names in the data set.\n",
    "        show_categories: bool, default False\n",
    "            Displays the unique classes and counts in each of the categorical feature in the data set.\n",
    "        plot_missing: bool, default True\n",
    "            Plots missing values as a heatmap\n",
    "        target: the target variable in the dataframe\n",
    "    Returns:\n",
    "    -------\n",
    "        None\n",
    "        \n",
    "        This function is stand alone use for quick statistical exploration of the data.\n",
    "    '''\n",
    "    \n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame or Series, got 'None'\")\n",
    "\n",
    "    ## Get categorical features\n",
    "    cat_features = get_cat_feats(data)\n",
    "    \n",
    "    #Get numerical features\n",
    "    num_features = get_num_feats(data)\n",
    "\n",
    "    print('First five data points')\n",
    "    display(data.head())\n",
    "    _space()\n",
    "\n",
    "    print('Random five data points')\n",
    "    display(data.sample(5))\n",
    "    _space()\n",
    "\n",
    "    print('Last five data points')\n",
    "    display(data.tail())\n",
    "    _space()\n",
    "\n",
    "    print('Shape of {} data set: {}'.format(name, data.shape))\n",
    "    _space()\n",
    "\n",
    "    print('Size of {} data set: {}'.format(name, data.size))\n",
    "    _space()\n",
    "\n",
    "    print('Data Types')\n",
    "    print(\"Note: All Non-numerical features are identified as objects in pandas\")\n",
    "    display(pd.DataFrame(data.dtypes, columns=['Data Type']))\n",
    "    _space()\n",
    "    \n",
    "    date_cols = get_date_cols(data)\n",
    "    if len(date_cols) is not 0:\n",
    "        print(\"Column(s) {} should be in Datetime format. Use the [to_date] function to convert to Pandas Datetime format\".format(date_cols))\n",
    "        _space()\n",
    "\n",
    "    print('Numerical Features in Data set')\n",
    "    print(num_features)\n",
    "    _space()\n",
    "\n",
    "    print('Categorical Features in Data set')\n",
    "    display(cat_features)\n",
    "    _space()\n",
    "\n",
    "    print('Statistical Description of Columns')\n",
    "    display(data.describe())\n",
    "    _space()\n",
    "    \n",
    "    print('Description of Categorical Features')\n",
    "    if cat_features != None:\n",
    "        display(data.describe(include=[np.object, pd.Categorical]).T)\n",
    "        _space()\n",
    "          \n",
    "    print('Unique class Count of Categorical features')\n",
    "    display(get_unique_counts(data))\n",
    "    _space()\n",
    "\n",
    "    if show_categories:     \n",
    "        print('Classes in Categorical Columns')\n",
    "        print(\"-\"*30)\n",
    "        class_count(data, cat_features)\n",
    "        _space()\n",
    "\n",
    "    print('Missing Values in Data')\n",
    "    display(display_missing(data))\n",
    "    _space()\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_processing(data=None, target=None, date_cols= None, percent = 70, method_encoding ='binary', method_scaling = 'scale'):\n",
    "    '''this function does feature engineering such as dealing with missing values, dropping redundant columns,\n",
    "        encoding categorical variables, creating new variables from the date columns, gives contigency_test, \n",
    "        print out the correlation between the encoded variables and the target variable.\n",
    "        It also scales the data using minmaxscaler.\n",
    "        \n",
    "        Parameters\n",
    "    ----------\n",
    "        date_cols: list/series/array\n",
    "            Date column names in the data set.\n",
    "        show_categories: bool, default False\n",
    "            Displays the unique classes and counts in each of the categorical feature in the data set.\n",
    "        plot_missing: bool, default True\n",
    "            Plots missing values as a heatmap\n",
    "        target: the target variable in the dataframe\n",
    "           The dependent variable in the dataset\n",
    "        percent: float, Default 70\n",
    "            Percentage of missing values to be in a column before it is eligible for removal.\n",
    "        method_encoding: the method for encoding variables before label-encoding them.\n",
    "            Value can be binary, onehot, sumencode\n",
    "        method_scaling: the method for scaling the dataset. The target variable is not scaled.\n",
    "            Value can be scale, normalize and normalize_outlier\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "        a dataframe of preprocessed columns. It is best to use this function after the text variables have been preprocessed.\n",
    "    This function is stand alone used for automatic feature engineering.\n",
    "    \n",
    "    It hash encodes the id and set it to be the index of the dataframe.\n",
    "    '''\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame/ numpy2d array, got 'None'\")\n",
    "    ###hash encoding id\n",
    "    cols = get_cat_feats(data)\n",
    "    new_col = []\n",
    "    for x in cols:\n",
    "        if re.search(\"_id\", x.lower()) and data[x].nunique()>3:\n",
    "            string = str(data[x])\n",
    "            if bool(re.findall(r'\\d+', string)) == True:\n",
    "                new_col.append(x)\n",
    "    if len(new_col)>0:\n",
    "        data.set_index(pd.util.hash_pandas_object(data[new_col]), drop=False, inplace=True)\n",
    "    data = data.drop(new_col, axis=1)\n",
    "    data = drop_redundant(data)\n",
    "    data = deal_with_missing_value(data, percent=70)\n",
    "    X = data.drop(target, axis =1)\n",
    "    y = data[target]\n",
    "    if date_cols is not None:\n",
    "        data_dates = data[date_cols]\n",
    "        data_dates = date_processing(data=data_dates, date_cols = date_cols)\n",
    "        data_dates = data_dates.drop(date_cols, axis=1)\n",
    "        X = data.drop(date_cols, axis=1)\n",
    "    \n",
    "    print('Bivariant Stats between categorical features')\n",
    "    print(bivariate_stats(X))\n",
    "    _space()\n",
    "    if target is not None:\n",
    "        print('Bivariant Stats between cat feats and target variable')\n",
    "        print(bivariate_stats_target(X, target))\n",
    "        _space()\n",
    "    X = encode_data(X, method=method_encoding)\n",
    "    if data[target].dtype == 'object':\n",
    "        data[target] = le.fit_transform(data[target].astype(str))\n",
    "    df = pd.concat([X, data_dates, data[target]], axis=1)\n",
    "    print('Pearson Correlation')\n",
    "    print(df.corr())\n",
    "    _space()\n",
    "    print('Kendall Correlation')\n",
    "    print(df.corr(method='kendall'))\n",
    "    _space()\n",
    "    print('Spearman Correlation')\n",
    "    print(df.corr(method='spearman'))\n",
    "    df = df[get_num_feats(data=df)]\n",
    "    corr = df.corr()\n",
    "    #fig, axe = plt.subplots(figsize=(12,9))\n",
    "    # fig = plt.figure(figsize=(200,14))\n",
    "    sns.set(rc={'figure.figsize':(18,18)}, font_scale=1.15)\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    with sns.axes_style('white'):\n",
    "        ax = sns.heatmap(\n",
    "            corr, \n",
    "            vmin=-1, vmax=1, center=0,\n",
    "            cmap='coolwarm',\n",
    "            square=True,\n",
    "            mask=mask,\n",
    "            annot=True,\n",
    "            fmt = '.2f'\n",
    "        )\n",
    "        ax.set_xticklabels(\n",
    "            ax.get_xticklabels(),\n",
    "            rotation=45,\n",
    "            horizontalalignment='right'\n",
    "        );\n",
    "    df2 = scale_normalize_data(df.drop(target, axis=1), cols = None, method = method_scaling)\n",
    "    df2 = pd.concat([df2, df[target].reset_index()], axis=1)\n",
    "    #df['hashed'] = df.index\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nmerge and join doesn't work correctly since there are similar columns in both datainput and datainput2. \\nThe correct way is to concat the two dataframes.\\n\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "datainput = pd.read_csv(r\"data2.csv\")\n",
    "datainput2 = pd.read_csv(r\"data1.csv\")\n",
    "\n",
    "''' \n",
    "merge and join doesn't work correctly since there are similar columns in both datainput and datainput2. \n",
    "The correct way is to concat the two dataframes.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datainput2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five data points\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>call_id</th>\n",
       "      <th>calls_account_id</th>\n",
       "      <th>date</th>\n",
       "      <th>channel</th>\n",
       "      <th>is_clm</th>\n",
       "      <th>activity_channel</th>\n",
       "      <th>key_message_id</th>\n",
       "      <th>content_category</th>\n",
       "      <th>content_message_local</th>\n",
       "      <th>...</th>\n",
       "      <th>period_week</th>\n",
       "      <th>period_month</th>\n",
       "      <th>con_size</th>\n",
       "      <th>geo_country</th>\n",
       "      <th>ISO3</th>\n",
       "      <th>ISO</th>\n",
       "      <th>areaAbbvie</th>\n",
       "      <th>geo_area</th>\n",
       "      <th>segment_concat</th>\n",
       "      <th>content_message_concat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a001t00000JKjL4AAL</td>\n",
       "      <td>a045J000002aHbrQAE</td>\n",
       "      <td>0011t00000IBwKeAAL</td>\n",
       "      <td>5/6/2020</td>\n",
       "      <td>E-mail</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5/4/2020</td>\n",
       "      <td>5/1/2020</td>\n",
       "      <td>True</td>\n",
       "      <td>Russia</td>\n",
       "      <td>RUS</td>\n",
       "      <td>RU</td>\n",
       "      <td>EEME&amp;A</td>\n",
       "      <td>EEMEA</td>\n",
       "      <td>B3@Rheum_Science_First_Green</td>\n",
       "      <td>(No message)@(No message)@(No message)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a001t00000JKjL4AAL</td>\n",
       "      <td>a041t00000OTN0RAAX</td>\n",
       "      <td>0011t00000IBwKeAAL</td>\n",
       "      <td>3/11/2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3/9/2020</td>\n",
       "      <td>3/1/2020</td>\n",
       "      <td>True</td>\n",
       "      <td>Russia</td>\n",
       "      <td>RUS</td>\n",
       "      <td>RU</td>\n",
       "      <td>EEME&amp;A</td>\n",
       "      <td>EEMEA</td>\n",
       "      <td>B3@Rheum_Science_First_Green</td>\n",
       "      <td>(No message)@(No message)@(No message)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a001t00000JKjL4AAL</td>\n",
       "      <td>a041t00000M6rqaAAB</td>\n",
       "      <td>0011t00000IBwKeAAL</td>\n",
       "      <td>1/14/2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1/13/2020</td>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>True</td>\n",
       "      <td>Russia</td>\n",
       "      <td>RUS</td>\n",
       "      <td>RU</td>\n",
       "      <td>EEME&amp;A</td>\n",
       "      <td>EEMEA</td>\n",
       "      <td>B3@Rheum_Science_First_Green</td>\n",
       "      <td>(No message)@(No message)@(No message)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a001t00000JKjL4AAL</td>\n",
       "      <td>a045J000003Lr7JQAS</td>\n",
       "      <td>0011t00000IBwKeAAL</td>\n",
       "      <td>5/14/2020</td>\n",
       "      <td>E-mail</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5/11/2020</td>\n",
       "      <td>5/1/2020</td>\n",
       "      <td>True</td>\n",
       "      <td>Russia</td>\n",
       "      <td>RUS</td>\n",
       "      <td>RU</td>\n",
       "      <td>EEME&amp;A</td>\n",
       "      <td>EEMEA</td>\n",
       "      <td>B3@Rheum_Science_First_Green</td>\n",
       "      <td>(No message)@(No message)@(No message)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a001t00000JKjL4AAL</td>\n",
       "      <td>a045J000002Y5coQAC</td>\n",
       "      <td>0011t00000IBwKeAAL</td>\n",
       "      <td>4/27/2020</td>\n",
       "      <td>E-mail</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4/27/2020</td>\n",
       "      <td>4/1/2020</td>\n",
       "      <td>True</td>\n",
       "      <td>Russia</td>\n",
       "      <td>RUS</td>\n",
       "      <td>RU</td>\n",
       "      <td>EEME&amp;A</td>\n",
       "      <td>EEMEA</td>\n",
       "      <td>B3@Rheum_Science_First_Green</td>\n",
       "      <td>(No message)@(No message)@(No message)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           product_id             call_id    calls_account_id       date  \\\n",
       "0  a001t00000JKjL4AAL  a045J000002aHbrQAE  0011t00000IBwKeAAL   5/6/2020   \n",
       "1  a001t00000JKjL4AAL  a041t00000OTN0RAAX  0011t00000IBwKeAAL  3/11/2020   \n",
       "2  a001t00000JKjL4AAL  a041t00000M6rqaAAB  0011t00000IBwKeAAL  1/14/2020   \n",
       "3  a001t00000JKjL4AAL  a045J000003Lr7JQAS  0011t00000IBwKeAAL  5/14/2020   \n",
       "4  a001t00000JKjL4AAL  a045J000002Y5coQAC  0011t00000IBwKeAAL  4/27/2020   \n",
       "\n",
       "  channel  is_clm  activity_channel key_message_id content_category  \\\n",
       "0  E-mail       0               NaN            NaN              NaN   \n",
       "1     NaN       0               NaN            NaN              NaN   \n",
       "2     NaN       0               NaN            NaN              NaN   \n",
       "3  E-mail       0               NaN            NaN              NaN   \n",
       "4  E-mail       0               NaN            NaN              NaN   \n",
       "\n",
       "  content_message_local                   ...                   period_week  \\\n",
       "0                   NaN                   ...                      5/4/2020   \n",
       "1                   NaN                   ...                      3/9/2020   \n",
       "2                   NaN                   ...                     1/13/2020   \n",
       "3                   NaN                   ...                     5/11/2020   \n",
       "4                   NaN                   ...                     4/27/2020   \n",
       "\n",
       "  period_month con_size geo_country ISO3 ISO areaAbbvie geo_area  \\\n",
       "0     5/1/2020     True      Russia  RUS  RU     EEME&A    EEMEA   \n",
       "1     3/1/2020     True      Russia  RUS  RU     EEME&A    EEMEA   \n",
       "2     1/1/2020     True      Russia  RUS  RU     EEME&A    EEMEA   \n",
       "3     5/1/2020     True      Russia  RUS  RU     EEME&A    EEMEA   \n",
       "4     4/1/2020     True      Russia  RUS  RU     EEME&A    EEMEA   \n",
       "\n",
       "                 segment_concat                  content_message_concat  \n",
       "0  B3@Rheum_Science_First_Green  (No message)@(No message)@(No message)  \n",
       "1  B3@Rheum_Science_First_Green  (No message)@(No message)@(No message)  \n",
       "2  B3@Rheum_Science_First_Green  (No message)@(No message)@(No message)  \n",
       "3  B3@Rheum_Science_First_Green  (No message)@(No message)@(No message)  \n",
       "4  B3@Rheum_Science_First_Green  (No message)@(No message)@(No message)  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Random five data points\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>call_id</th>\n",
       "      <th>calls_account_id</th>\n",
       "      <th>date</th>\n",
       "      <th>channel</th>\n",
       "      <th>is_clm</th>\n",
       "      <th>activity_channel</th>\n",
       "      <th>key_message_id</th>\n",
       "      <th>content_category</th>\n",
       "      <th>content_message_local</th>\n",
       "      <th>...</th>\n",
       "      <th>period_week</th>\n",
       "      <th>period_month</th>\n",
       "      <th>con_size</th>\n",
       "      <th>geo_country</th>\n",
       "      <th>ISO3</th>\n",
       "      <th>ISO</th>\n",
       "      <th>areaAbbvie</th>\n",
       "      <th>geo_area</th>\n",
       "      <th>segment_concat</th>\n",
       "      <th>content_message_concat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42887</th>\n",
       "      <td>a001v00002KP02YAAT</td>\n",
       "      <td>a041v000015TJ5mAAG</td>\n",
       "      <td>001G000001kKZnZIAW</td>\n",
       "      <td>3/4/2020</td>\n",
       "      <td>Face to Face</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a0D1v00000NqqzpEAB</td>\n",
       "      <td>Efficacy</td>\n",
       "      <td>Hhere Remission Rinvoq+MTX vs. Ada+ MTX</td>\n",
       "      <td>...</td>\n",
       "      <td>3/2/2020</td>\n",
       "      <td>3/1/2020</td>\n",
       "      <td>True</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>CHE</td>\n",
       "      <td>CH</td>\n",
       "      <td>WE&amp;C</td>\n",
       "      <td>WEC</td>\n",
       "      <td>B1@Rheum_Science First_Green</td>\n",
       "      <td>Efficacy@Remission rates vs placebo + MTX and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54656</th>\n",
       "      <td>a001v00002KP02YAAT</td>\n",
       "      <td>a041v00001BOutIAAT</td>\n",
       "      <td>001G000001sGl7YIAS</td>\n",
       "      <td>1/20/2020</td>\n",
       "      <td>Face to Face</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a0D1v00000NqTGiEAN</td>\n",
       "      <td>Efficacy</td>\n",
       "      <td>REMISSION: noch nie dagewesene Remissionsraten...</td>\n",
       "      <td>...</td>\n",
       "      <td>1/20/2020</td>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>True</td>\n",
       "      <td>Austria</td>\n",
       "      <td>AUT</td>\n",
       "      <td>AT</td>\n",
       "      <td>WE&amp;C</td>\n",
       "      <td>WEC</td>\n",
       "      <td>B2@Rheum_Trust First_Yellow</td>\n",
       "      <td>Efficacy@Remission rates vs placebo + MTX and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51644</th>\n",
       "      <td>a001t00000JKjL4AAL</td>\n",
       "      <td>a045J000002Zh9GQAS</td>\n",
       "      <td>0016F00001g2970QAA</td>\n",
       "      <td>5/1/2020</td>\n",
       "      <td>E-mail</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a0D1t000005jzT0EAI</td>\n",
       "      <td>Efficacy</td>\n",
       "      <td>Proven superiority vs ADA (ACR 50, Pain and HAQ)</td>\n",
       "      <td>...</td>\n",
       "      <td>4/27/2020</td>\n",
       "      <td>5/1/2020</td>\n",
       "      <td>True</td>\n",
       "      <td>Australia</td>\n",
       "      <td>AUS</td>\n",
       "      <td>AU</td>\n",
       "      <td>JAPAC</td>\n",
       "      <td>JAPAC</td>\n",
       "      <td>(No segment)@(No segment)</td>\n",
       "      <td>Efficacy@ACR50, pain, and HAQ-DI vs ADA + MTX@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55874</th>\n",
       "      <td>a001t00000JKjL4AAL</td>\n",
       "      <td>a041t00000Obyh3AAB</td>\n",
       "      <td>0011t00000TazWmAAJ</td>\n",
       "      <td>3/3/2020</td>\n",
       "      <td>Face to Face</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a0D1t000005k7VjEAI</td>\n",
       "      <td>Safety</td>\n",
       "      <td>Safety profile: robust trial program with 4000...</td>\n",
       "      <td>...</td>\n",
       "      <td>3/2/2020</td>\n",
       "      <td>3/1/2020</td>\n",
       "      <td>True</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>BRA</td>\n",
       "      <td>BR</td>\n",
       "      <td>LATAM</td>\n",
       "      <td>LATAM</td>\n",
       "      <td>X@(No segment)</td>\n",
       "      <td>Safety@Safety profile: robust trial program wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21427</th>\n",
       "      <td>a001v00002KP02YAAT</td>\n",
       "      <td>a041v00001BOyllAAD</td>\n",
       "      <td>001G000000vSv0JIAS</td>\n",
       "      <td>1/21/2020</td>\n",
       "      <td>Face to Face</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a0D1v00000NqeG5EAJ</td>\n",
       "      <td>Safety</td>\n",
       "      <td>Tolrance tablie au travers du programme SELE...</td>\n",
       "      <td>...</td>\n",
       "      <td>1/20/2020</td>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>True</td>\n",
       "      <td>France</td>\n",
       "      <td>FRA</td>\n",
       "      <td>FR</td>\n",
       "      <td>WE&amp;C</td>\n",
       "      <td>WEC</td>\n",
       "      <td>C2@Blue Segment</td>\n",
       "      <td>Safety@Safety profile: robust trial program wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               product_id             call_id    calls_account_id       date  \\\n",
       "42887  a001v00002KP02YAAT  a041v000015TJ5mAAG  001G000001kKZnZIAW   3/4/2020   \n",
       "54656  a001v00002KP02YAAT  a041v00001BOutIAAT  001G000001sGl7YIAS  1/20/2020   \n",
       "51644  a001t00000JKjL4AAL  a045J000002Zh9GQAS  0016F00001g2970QAA   5/1/2020   \n",
       "55874  a001t00000JKjL4AAL  a041t00000Obyh3AAB  0011t00000TazWmAAJ   3/3/2020   \n",
       "21427  a001v00002KP02YAAT  a041v00001BOyllAAD  001G000000vSv0JIAS  1/21/2020   \n",
       "\n",
       "            channel  is_clm  activity_channel      key_message_id  \\\n",
       "42887  Face to Face       0               NaN  a0D1v00000NqqzpEAB   \n",
       "54656  Face to Face       1               NaN  a0D1v00000NqTGiEAN   \n",
       "51644        E-mail       0               NaN  a0D1t000005jzT0EAI   \n",
       "55874  Face to Face       1               NaN  a0D1t000005k7VjEAI   \n",
       "21427  Face to Face       0               NaN  a0D1v00000NqeG5EAJ   \n",
       "\n",
       "      content_category                              content_message_local  \\\n",
       "42887         Efficacy           Hhere Remission Rinvoq+MTX vs. Ada+ MTX   \n",
       "54656         Efficacy  REMISSION: noch nie dagewesene Remissionsraten...   \n",
       "51644         Efficacy   Proven superiority vs ADA (ACR 50, Pain and HAQ)   \n",
       "55874           Safety  Safety profile: robust trial program with 4000...   \n",
       "21427           Safety  Tolrance tablie au travers du programme SELE...   \n",
       "\n",
       "                             ...                         period_week  \\\n",
       "42887                        ...                            3/2/2020   \n",
       "54656                        ...                           1/20/2020   \n",
       "51644                        ...                           4/27/2020   \n",
       "55874                        ...                            3/2/2020   \n",
       "21427                        ...                           1/20/2020   \n",
       "\n",
       "      period_month con_size  geo_country ISO3 ISO areaAbbvie geo_area  \\\n",
       "42887     3/1/2020     True  Switzerland  CHE  CH       WE&C      WEC   \n",
       "54656     1/1/2020     True      Austria  AUT  AT       WE&C      WEC   \n",
       "51644     5/1/2020     True    Australia  AUS  AU      JAPAC    JAPAC   \n",
       "55874     3/1/2020     True       Brazil  BRA  BR      LATAM    LATAM   \n",
       "21427     1/1/2020     True       France  FRA  FR       WE&C      WEC   \n",
       "\n",
       "                     segment_concat  \\\n",
       "42887  B1@Rheum_Science First_Green   \n",
       "54656   B2@Rheum_Trust First_Yellow   \n",
       "51644     (No segment)@(No segment)   \n",
       "55874                X@(No segment)   \n",
       "21427               C2@Blue Segment   \n",
       "\n",
       "                                  content_message_concat  \n",
       "42887  Efficacy@Remission rates vs placebo + MTX and ...  \n",
       "54656  Efficacy@Remission rates vs placebo + MTX and ...  \n",
       "51644  Efficacy@ACR50, pain, and HAQ-DI vs ADA + MTX@...  \n",
       "55874  Safety@Safety profile: robust trial program wi...  \n",
       "21427  Safety@Safety profile: robust trial program wi...  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Last five data points\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>call_id</th>\n",
       "      <th>calls_account_id</th>\n",
       "      <th>date</th>\n",
       "      <th>channel</th>\n",
       "      <th>is_clm</th>\n",
       "      <th>activity_channel</th>\n",
       "      <th>key_message_id</th>\n",
       "      <th>content_category</th>\n",
       "      <th>content_message_local</th>\n",
       "      <th>...</th>\n",
       "      <th>period_week</th>\n",
       "      <th>period_month</th>\n",
       "      <th>con_size</th>\n",
       "      <th>geo_country</th>\n",
       "      <th>ISO3</th>\n",
       "      <th>ISO</th>\n",
       "      <th>areaAbbvie</th>\n",
       "      <th>geo_area</th>\n",
       "      <th>segment_concat</th>\n",
       "      <th>content_message_concat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168528</th>\n",
       "      <td>a001v00002W6XH7AAN</td>\n",
       "      <td>a041v00001Cm2yjAAB</td>\n",
       "      <td>0011v00001vN9SKAA0</td>\n",
       "      <td>2/25/2020</td>\n",
       "      <td>Face to Face</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a0D1v00000NqhvhEAB</td>\n",
       "      <td>Efficacy</td>\n",
       "      <td>RINVOQ+MTX erstmals signifikant hoehere Remiss...</td>\n",
       "      <td>...</td>\n",
       "      <td>2/24/2020</td>\n",
       "      <td>2/1/2020</td>\n",
       "      <td>True</td>\n",
       "      <td>Germany</td>\n",
       "      <td>DEU</td>\n",
       "      <td>DE</td>\n",
       "      <td>WE&amp;C</td>\n",
       "      <td>WEC</td>\n",
       "      <td>(No segment)@(No segment)</td>\n",
       "      <td>Efficacy@Remission rates vs placebo + MTX and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168529</th>\n",
       "      <td>a001v00002W6XH7AAN</td>\n",
       "      <td>a041v00001CkmZ4AAJ</td>\n",
       "      <td>0011v00001vN9SKAA0</td>\n",
       "      <td>2/19/2020</td>\n",
       "      <td>Face to Face</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2/17/2020</td>\n",
       "      <td>2/1/2020</td>\n",
       "      <td>True</td>\n",
       "      <td>Germany</td>\n",
       "      <td>DEU</td>\n",
       "      <td>DE</td>\n",
       "      <td>WE&amp;C</td>\n",
       "      <td>WEC</td>\n",
       "      <td>(No segment)@(No segment)</td>\n",
       "      <td>(No message)@(No message)@(No message)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168530</th>\n",
       "      <td>a001v00002W6XH7AAN</td>\n",
       "      <td>a041v00001ENvF8AAL</td>\n",
       "      <td>0011v00001vN9SKAA0</td>\n",
       "      <td>3/24/2020</td>\n",
       "      <td>Telephone</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3/23/2020</td>\n",
       "      <td>3/1/2020</td>\n",
       "      <td>True</td>\n",
       "      <td>Germany</td>\n",
       "      <td>DEU</td>\n",
       "      <td>DE</td>\n",
       "      <td>WE&amp;C</td>\n",
       "      <td>WEC</td>\n",
       "      <td>(No segment)@(No segment)</td>\n",
       "      <td>(No message)@(No message)@(No message)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168531</th>\n",
       "      <td>a001v00002W6XH7AAN</td>\n",
       "      <td>a041v00001CmGRQAA3</td>\n",
       "      <td>0011v00001vN9SKAA0</td>\n",
       "      <td>2/26/2020</td>\n",
       "      <td>Face to Face</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2/24/2020</td>\n",
       "      <td>2/1/2020</td>\n",
       "      <td>True</td>\n",
       "      <td>Germany</td>\n",
       "      <td>DEU</td>\n",
       "      <td>DE</td>\n",
       "      <td>WE&amp;C</td>\n",
       "      <td>WEC</td>\n",
       "      <td>(No segment)@(No segment)</td>\n",
       "      <td>(No message)@(No message)@(No message)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168532</th>\n",
       "      <td>a001v00002W6XH7AAN</td>\n",
       "      <td>a041v00001IKqh9AAD</td>\n",
       "      <td>0011v00001vN9SKAA0</td>\n",
       "      <td>6/17/2020</td>\n",
       "      <td>Face to Face</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6/15/2020</td>\n",
       "      <td>6/1/2020</td>\n",
       "      <td>True</td>\n",
       "      <td>Germany</td>\n",
       "      <td>DEU</td>\n",
       "      <td>DE</td>\n",
       "      <td>WE&amp;C</td>\n",
       "      <td>WEC</td>\n",
       "      <td>(No segment)@(No segment)</td>\n",
       "      <td>(No message)@(No message)@(No message)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                product_id             call_id    calls_account_id       date  \\\n",
       "168528  a001v00002W6XH7AAN  a041v00001Cm2yjAAB  0011v00001vN9SKAA0  2/25/2020   \n",
       "168529  a001v00002W6XH7AAN  a041v00001CkmZ4AAJ  0011v00001vN9SKAA0  2/19/2020   \n",
       "168530  a001v00002W6XH7AAN  a041v00001ENvF8AAL  0011v00001vN9SKAA0  3/24/2020   \n",
       "168531  a001v00002W6XH7AAN  a041v00001CmGRQAA3  0011v00001vN9SKAA0  2/26/2020   \n",
       "168532  a001v00002W6XH7AAN  a041v00001IKqh9AAD  0011v00001vN9SKAA0  6/17/2020   \n",
       "\n",
       "             channel  is_clm  activity_channel      key_message_id  \\\n",
       "168528  Face to Face       1               NaN  a0D1v00000NqhvhEAB   \n",
       "168529  Face to Face       0               NaN                 NaN   \n",
       "168530     Telephone       0               NaN                 NaN   \n",
       "168531  Face to Face       0               NaN                 NaN   \n",
       "168532  Face to Face       0               NaN                 NaN   \n",
       "\n",
       "       content_category                              content_message_local  \\\n",
       "168528         Efficacy  RINVOQ+MTX erstmals signifikant hoehere Remiss...   \n",
       "168529              NaN                                                NaN   \n",
       "168530              NaN                                                NaN   \n",
       "168531              NaN                                                NaN   \n",
       "168532              NaN                                                NaN   \n",
       "\n",
       "                              ...                         period_week  \\\n",
       "168528                        ...                           2/24/2020   \n",
       "168529                        ...                           2/17/2020   \n",
       "168530                        ...                           3/23/2020   \n",
       "168531                        ...                           2/24/2020   \n",
       "168532                        ...                           6/15/2020   \n",
       "\n",
       "       period_month con_size geo_country ISO3 ISO areaAbbvie geo_area  \\\n",
       "168528     2/1/2020     True     Germany  DEU  DE       WE&C      WEC   \n",
       "168529     2/1/2020     True     Germany  DEU  DE       WE&C      WEC   \n",
       "168530     3/1/2020     True     Germany  DEU  DE       WE&C      WEC   \n",
       "168531     2/1/2020     True     Germany  DEU  DE       WE&C      WEC   \n",
       "168532     6/1/2020     True     Germany  DEU  DE       WE&C      WEC   \n",
       "\n",
       "                   segment_concat  \\\n",
       "168528  (No segment)@(No segment)   \n",
       "168529  (No segment)@(No segment)   \n",
       "168530  (No segment)@(No segment)   \n",
       "168531  (No segment)@(No segment)   \n",
       "168532  (No segment)@(No segment)   \n",
       "\n",
       "                                   content_message_concat  \n",
       "168528  Efficacy@Remission rates vs placebo + MTX and ...  \n",
       "168529             (No message)@(No message)@(No message)  \n",
       "168530             (No message)@(No message)@(No message)  \n",
       "168531             (No message)@(No message)@(No message)  \n",
       "168532             (No message)@(No message)@(No message)  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Shape of  data set: (168533, 35)\n",
      "\n",
      "\n",
      "Size of  data set: 5898655\n",
      "\n",
      "\n",
      "Data Types\n",
      "Note: All Non-numerical features are identified as objects in pandas\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>product_id</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>call_id</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calls_account_id</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>channel</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_clm</th>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activity_channel</th>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key_message_id</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content_category</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content_message_local</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>local_key_message_vault_id</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content_message_global</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_key_message_vault_id</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>call_date</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indication</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>therapeutic_area</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fs_product_id</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>account_id</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>segment_quant</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>segment_qual</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>change_clm</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>touchpoint_channel_clm</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>measure_source</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>period_week</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>period_month</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>con_size</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geo_country</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISO3</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISO</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>areaAbbvie</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geo_area</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>segment_concat</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content_message_concat</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Data Type\n",
       "product_id                     object\n",
       "call_id                        object\n",
       "calls_account_id               object\n",
       "date                           object\n",
       "channel                        object\n",
       "is_clm                          int64\n",
       "activity_channel              float64\n",
       "key_message_id                 object\n",
       "content_category               object\n",
       "content_message_local          object\n",
       "local_key_message_vault_id     object\n",
       "content_message_global         object\n",
       "global_key_message_vault_id    object\n",
       "id                             object\n",
       "call_date                      object\n",
       "product                        object\n",
       "indication                     object\n",
       "therapeutic_area               object\n",
       "fs_product_id                  object\n",
       "account_id                     object\n",
       "segment_quant                  object\n",
       "segment_qual                   object\n",
       "change_clm                     object\n",
       "touchpoint_channel_clm         object\n",
       "measure_source                 object\n",
       "period_week                    object\n",
       "period_month                   object\n",
       "con_size                         bool\n",
       "geo_country                    object\n",
       "ISO3                           object\n",
       "ISO                            object\n",
       "areaAbbvie                     object\n",
       "geo_area                       object\n",
       "segment_concat                 object\n",
       "content_message_concat         object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Numerical Features in Data set\n",
      "['is_clm', 'activity_channel']\n",
      "\n",
      "\n",
      "Categorical Features in Data set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['product_id',\n",
       " 'call_id',\n",
       " 'calls_account_id',\n",
       " 'date',\n",
       " 'channel',\n",
       " 'key_message_id',\n",
       " 'content_category',\n",
       " 'content_message_local',\n",
       " 'local_key_message_vault_id',\n",
       " 'content_message_global',\n",
       " 'global_key_message_vault_id',\n",
       " 'id',\n",
       " 'call_date',\n",
       " 'product',\n",
       " 'indication',\n",
       " 'therapeutic_area',\n",
       " 'fs_product_id',\n",
       " 'account_id',\n",
       " 'segment_quant',\n",
       " 'segment_qual',\n",
       " 'change_clm',\n",
       " 'touchpoint_channel_clm',\n",
       " 'measure_source',\n",
       " 'period_week',\n",
       " 'period_month',\n",
       " 'geo_country',\n",
       " 'ISO3',\n",
       " 'ISO',\n",
       " 'areaAbbvie',\n",
       " 'geo_area',\n",
       " 'segment_concat',\n",
       " 'content_message_concat']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Statistical Description of Columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_clm</th>\n",
       "      <th>activity_channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>168533.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.613138</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.487033</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              is_clm  activity_channel\n",
       "count  168533.000000               0.0\n",
       "mean        0.613138               NaN\n",
       "std         0.487033               NaN\n",
       "min         0.000000               NaN\n",
       "25%         0.000000               NaN\n",
       "50%         1.000000               NaN\n",
       "75%         1.000000               NaN\n",
       "max         1.000000               NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Description of Categorical Features\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>product_id</th>\n",
       "      <td>168533</td>\n",
       "      <td>4</td>\n",
       "      <td>a001v00002KP02YAAT</td>\n",
       "      <td>121998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>call_id</th>\n",
       "      <td>168533</td>\n",
       "      <td>71885</td>\n",
       "      <td>a045J000002aVQpQAM</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calls_account_id</th>\n",
       "      <td>166859</td>\n",
       "      <td>14790</td>\n",
       "      <td>0016F00001lcQ4KQAU</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>168533</td>\n",
       "      <td>195</td>\n",
       "      <td>3/4/2020</td>\n",
       "      <td>2924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>channel</th>\n",
       "      <td>159635</td>\n",
       "      <td>5</td>\n",
       "      <td>Face to Face</td>\n",
       "      <td>104271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key_message_id</th>\n",
       "      <td>125150</td>\n",
       "      <td>1258</td>\n",
       "      <td>a0D1v00000NqhvhEAB</td>\n",
       "      <td>7256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content_category</th>\n",
       "      <td>113301</td>\n",
       "      <td>7</td>\n",
       "      <td>Efficacy</td>\n",
       "      <td>60338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content_message_local</th>\n",
       "      <td>113304</td>\n",
       "      <td>122</td>\n",
       "      <td>RINVOQ+MTX erstmals signifikant hoehere Remiss...</td>\n",
       "      <td>18491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>local_key_message_vault_id</th>\n",
       "      <td>113304</td>\n",
       "      <td>165</td>\n",
       "      <td>V1N000000000501</td>\n",
       "      <td>18491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content_message_global</th>\n",
       "      <td>113304</td>\n",
       "      <td>16</td>\n",
       "      <td>Remission rates vs placebo + MTX and ADA +MTX</td>\n",
       "      <td>40687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_key_message_vault_id</th>\n",
       "      <td>113304</td>\n",
       "      <td>14</td>\n",
       "      <td>V1M000000000101</td>\n",
       "      <td>40687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>125150</td>\n",
       "      <td>122790</td>\n",
       "      <td>a081v00001jtqEaAAI</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>call_date</th>\n",
       "      <td>125150</td>\n",
       "      <td>155</td>\n",
       "      <td>3/10/2020</td>\n",
       "      <td>2759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product</th>\n",
       "      <td>168533</td>\n",
       "      <td>3</td>\n",
       "      <td>Rinvoq_RA</td>\n",
       "      <td>159994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indication</th>\n",
       "      <td>159994</td>\n",
       "      <td>1</td>\n",
       "      <td>Rheumatoid Arthritis (RA)</td>\n",
       "      <td>159994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>therapeutic_area</th>\n",
       "      <td>168533</td>\n",
       "      <td>1</td>\n",
       "      <td>Rheumatology</td>\n",
       "      <td>168533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fs_product_id</th>\n",
       "      <td>135374</td>\n",
       "      <td>2</td>\n",
       "      <td>a001v00002KP02YAAT</td>\n",
       "      <td>100644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>account_id</th>\n",
       "      <td>135374</td>\n",
       "      <td>10904</td>\n",
       "      <td>0016F00001lcQ4KQAU</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>segment_quant</th>\n",
       "      <td>131277</td>\n",
       "      <td>16</td>\n",
       "      <td>A1</td>\n",
       "      <td>23698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>segment_qual</th>\n",
       "      <td>91960</td>\n",
       "      <td>33</td>\n",
       "      <td>Rheum_Science First_Green</td>\n",
       "      <td>28698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>change_clm</th>\n",
       "      <td>168533</td>\n",
       "      <td>2</td>\n",
       "      <td>CLM</td>\n",
       "      <td>103334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>touchpoint_channel_clm</th>\n",
       "      <td>168533</td>\n",
       "      <td>12</td>\n",
       "      <td>Face to Face@1</td>\n",
       "      <td>82968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>measure_source</th>\n",
       "      <td>168533</td>\n",
       "      <td>1</td>\n",
       "      <td>veeva_interactions</td>\n",
       "      <td>168533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>period_week</th>\n",
       "      <td>168533</td>\n",
       "      <td>51</td>\n",
       "      <td>3/2/2020</td>\n",
       "      <td>12945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>period_month</th>\n",
       "      <td>168533</td>\n",
       "      <td>15</td>\n",
       "      <td>2/1/2020</td>\n",
       "      <td>38365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geo_country</th>\n",
       "      <td>168533</td>\n",
       "      <td>27</td>\n",
       "      <td>Germany</td>\n",
       "      <td>60334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISO3</th>\n",
       "      <td>168533</td>\n",
       "      <td>27</td>\n",
       "      <td>DEU</td>\n",
       "      <td>60334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISO</th>\n",
       "      <td>168533</td>\n",
       "      <td>27</td>\n",
       "      <td>DE</td>\n",
       "      <td>60334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>areaAbbvie</th>\n",
       "      <td>168533</td>\n",
       "      <td>4</td>\n",
       "      <td>WE&amp;C</td>\n",
       "      <td>130537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geo_area</th>\n",
       "      <td>168533</td>\n",
       "      <td>4</td>\n",
       "      <td>WEC</td>\n",
       "      <td>130537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>segment_concat</th>\n",
       "      <td>168533</td>\n",
       "      <td>230</td>\n",
       "      <td>(No segment)@(No segment)</td>\n",
       "      <td>35027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content_message_concat</th>\n",
       "      <td>168533</td>\n",
       "      <td>126</td>\n",
       "      <td>(No message)@(No message)@(No message)</td>\n",
       "      <td>67167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              count  unique  \\\n",
       "product_id                   168533       4   \n",
       "call_id                      168533   71885   \n",
       "calls_account_id             166859   14790   \n",
       "date                         168533     195   \n",
       "channel                      159635       5   \n",
       "key_message_id               125150    1258   \n",
       "content_category             113301       7   \n",
       "content_message_local        113304     122   \n",
       "local_key_message_vault_id   113304     165   \n",
       "content_message_global       113304      16   \n",
       "global_key_message_vault_id  113304      14   \n",
       "id                           125150  122790   \n",
       "call_date                    125150     155   \n",
       "product                      168533       3   \n",
       "indication                   159994       1   \n",
       "therapeutic_area             168533       1   \n",
       "fs_product_id                135374       2   \n",
       "account_id                   135374   10904   \n",
       "segment_quant                131277      16   \n",
       "segment_qual                  91960      33   \n",
       "change_clm                   168533       2   \n",
       "touchpoint_channel_clm       168533      12   \n",
       "measure_source               168533       1   \n",
       "period_week                  168533      51   \n",
       "period_month                 168533      15   \n",
       "geo_country                  168533      27   \n",
       "ISO3                         168533      27   \n",
       "ISO                          168533      27   \n",
       "areaAbbvie                   168533       4   \n",
       "geo_area                     168533       4   \n",
       "segment_concat               168533     230   \n",
       "content_message_concat       168533     126   \n",
       "\n",
       "                                                                           top  \\\n",
       "product_id                                                  a001v00002KP02YAAT   \n",
       "call_id                                                     a045J000002aVQpQAM   \n",
       "calls_account_id                                            0016F00001lcQ4KQAU   \n",
       "date                                                                  3/4/2020   \n",
       "channel                                                           Face to Face   \n",
       "key_message_id                                              a0D1v00000NqhvhEAB   \n",
       "content_category                                                      Efficacy   \n",
       "content_message_local        RINVOQ+MTX erstmals signifikant hoehere Remiss...   \n",
       "local_key_message_vault_id                                     V1N000000000501   \n",
       "content_message_global           Remission rates vs placebo + MTX and ADA +MTX   \n",
       "global_key_message_vault_id                                    V1M000000000101   \n",
       "id                                                          a081v00001jtqEaAAI   \n",
       "call_date                                                            3/10/2020   \n",
       "product                                                              Rinvoq_RA   \n",
       "indication                                           Rheumatoid Arthritis (RA)   \n",
       "therapeutic_area                                                  Rheumatology   \n",
       "fs_product_id                                               a001v00002KP02YAAT   \n",
       "account_id                                                  0016F00001lcQ4KQAU   \n",
       "segment_quant                                                               A1   \n",
       "segment_qual                                         Rheum_Science First_Green   \n",
       "change_clm                                                                 CLM   \n",
       "touchpoint_channel_clm                                          Face to Face@1   \n",
       "measure_source                                              veeva_interactions   \n",
       "period_week                                                           3/2/2020   \n",
       "period_month                                                          2/1/2020   \n",
       "geo_country                                                            Germany   \n",
       "ISO3                                                                       DEU   \n",
       "ISO                                                                         DE   \n",
       "areaAbbvie                                                                WE&C   \n",
       "geo_area                                                                   WEC   \n",
       "segment_concat                                       (No segment)@(No segment)   \n",
       "content_message_concat                  (No message)@(No message)@(No message)   \n",
       "\n",
       "                               freq  \n",
       "product_id                   121998  \n",
       "call_id                         286  \n",
       "calls_account_id                359  \n",
       "date                           2924  \n",
       "channel                      104271  \n",
       "key_message_id                 7256  \n",
       "content_category              60338  \n",
       "content_message_local         18491  \n",
       "local_key_message_vault_id    18491  \n",
       "content_message_global        40687  \n",
       "global_key_message_vault_id   40687  \n",
       "id                                2  \n",
       "call_date                      2759  \n",
       "product                      159994  \n",
       "indication                   159994  \n",
       "therapeutic_area             168533  \n",
       "fs_product_id                100644  \n",
       "account_id                      359  \n",
       "segment_quant                 23698  \n",
       "segment_qual                  28698  \n",
       "change_clm                   103334  \n",
       "touchpoint_channel_clm        82968  \n",
       "measure_source               168533  \n",
       "period_week                   12945  \n",
       "period_month                  38365  \n",
       "geo_country                   60334  \n",
       "ISO3                          60334  \n",
       "ISO                           60334  \n",
       "areaAbbvie                   130537  \n",
       "geo_area                     130537  \n",
       "segment_concat                35027  \n",
       "content_message_concat        67167  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Unique class Count of Categorical features\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_45493940_d617_11ea_8310_a44cc85d0dd2row0_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row1_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row2_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row3_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row4_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row5_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row6_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row7_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row8_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row9_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row10_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row11_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row12_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row13_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row14_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row15_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row16_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row17_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row18_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row19_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row20_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row21_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row22_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row23_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row24_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row25_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row26_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row27_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row28_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row29_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row30_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }    #T_45493940_d617_11ea_8310_a44cc85d0dd2row31_col1 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 0%, transparent 0.0%, #d65f5f 0.0%, #d65f5f 0.0%, transparent 0.0%);\n",
       "        }</style>  \n",
       "<table id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2\" > \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >Feature</th> \n",
       "        <th class=\"col_heading level0 col1\" >Unique Count</th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row0\" class=\"row_heading level0 row0\" >0</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row0_col0\" class=\"data row0 col0\" >product_id</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row0_col1\" class=\"data row0 col1\" >4</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row1\" class=\"row_heading level0 row1\" >1</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row1_col0\" class=\"data row1 col0\" >call_id</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row1_col1\" class=\"data row1 col1\" >71885</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row2\" class=\"row_heading level0 row2\" >2</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row2_col0\" class=\"data row2 col0\" >calls_account_id</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row2_col1\" class=\"data row2 col1\" >14791</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row3\" class=\"row_heading level0 row3\" >3</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row3_col0\" class=\"data row3 col0\" >date</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row3_col1\" class=\"data row3 col1\" >195</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row4\" class=\"row_heading level0 row4\" >4</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row4_col0\" class=\"data row4 col0\" >channel</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row4_col1\" class=\"data row4 col1\" >6</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row5\" class=\"row_heading level0 row5\" >5</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row5_col0\" class=\"data row5 col0\" >key_message_id</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row5_col1\" class=\"data row5 col1\" >1259</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row6\" class=\"row_heading level0 row6\" >6</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row6_col0\" class=\"data row6 col0\" >content_category</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row6_col1\" class=\"data row6 col1\" >8</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row7\" class=\"row_heading level0 row7\" >7</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row7_col0\" class=\"data row7 col0\" >content_message_local</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row7_col1\" class=\"data row7 col1\" >123</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row8\" class=\"row_heading level0 row8\" >8</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row8_col0\" class=\"data row8 col0\" >local_key_message_vault_id</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row8_col1\" class=\"data row8 col1\" >166</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row9\" class=\"row_heading level0 row9\" >9</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row9_col0\" class=\"data row9 col0\" >content_message_global</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row9_col1\" class=\"data row9 col1\" >17</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row10\" class=\"row_heading level0 row10\" >10</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row10_col0\" class=\"data row10 col0\" >global_key_message_vault_id</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row10_col1\" class=\"data row10 col1\" >15</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row11\" class=\"row_heading level0 row11\" >11</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row11_col0\" class=\"data row11 col0\" >id</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row11_col1\" class=\"data row11 col1\" >122791</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row12\" class=\"row_heading level0 row12\" >12</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row12_col0\" class=\"data row12 col0\" >call_date</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row12_col1\" class=\"data row12 col1\" >156</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row13\" class=\"row_heading level0 row13\" >13</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row13_col0\" class=\"data row13 col0\" >product</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row13_col1\" class=\"data row13 col1\" >3</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row14\" class=\"row_heading level0 row14\" >14</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row14_col0\" class=\"data row14 col0\" >indication</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row14_col1\" class=\"data row14 col1\" >2</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row15\" class=\"row_heading level0 row15\" >15</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row15_col0\" class=\"data row15 col0\" >therapeutic_area</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row15_col1\" class=\"data row15 col1\" >1</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row16\" class=\"row_heading level0 row16\" >16</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row16_col0\" class=\"data row16 col0\" >fs_product_id</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row16_col1\" class=\"data row16 col1\" >3</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row17\" class=\"row_heading level0 row17\" >17</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row17_col0\" class=\"data row17 col0\" >account_id</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row17_col1\" class=\"data row17 col1\" >10905</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row18\" class=\"row_heading level0 row18\" >18</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row18_col0\" class=\"data row18 col0\" >segment_quant</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row18_col1\" class=\"data row18 col1\" >17</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row19\" class=\"row_heading level0 row19\" >19</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row19_col0\" class=\"data row19 col0\" >segment_qual</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row19_col1\" class=\"data row19 col1\" >34</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row20\" class=\"row_heading level0 row20\" >20</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row20_col0\" class=\"data row20 col0\" >change_clm</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row20_col1\" class=\"data row20 col1\" >2</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row21\" class=\"row_heading level0 row21\" >21</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row21_col0\" class=\"data row21 col0\" >touchpoint_channel_clm</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row21_col1\" class=\"data row21 col1\" >12</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row22\" class=\"row_heading level0 row22\" >22</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row22_col0\" class=\"data row22 col0\" >measure_source</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row22_col1\" class=\"data row22 col1\" >1</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row23\" class=\"row_heading level0 row23\" >23</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row23_col0\" class=\"data row23 col0\" >period_week</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row23_col1\" class=\"data row23 col1\" >51</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row24\" class=\"row_heading level0 row24\" >24</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row24_col0\" class=\"data row24 col0\" >period_month</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row24_col1\" class=\"data row24 col1\" >15</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row25\" class=\"row_heading level0 row25\" >25</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row25_col0\" class=\"data row25 col0\" >geo_country</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row25_col1\" class=\"data row25 col1\" >27</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row26\" class=\"row_heading level0 row26\" >26</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row26_col0\" class=\"data row26 col0\" >ISO3</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row26_col1\" class=\"data row26 col1\" >27</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row27\" class=\"row_heading level0 row27\" >27</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row27_col0\" class=\"data row27 col0\" >ISO</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row27_col1\" class=\"data row27 col1\" >27</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row28\" class=\"row_heading level0 row28\" >28</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row28_col0\" class=\"data row28 col0\" >areaAbbvie</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row28_col1\" class=\"data row28 col1\" >4</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row29\" class=\"row_heading level0 row29\" >29</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row29_col0\" class=\"data row29 col0\" >geo_area</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row29_col1\" class=\"data row29 col1\" >4</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row30\" class=\"row_heading level0 row30\" >30</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row30_col0\" class=\"data row30 col0\" >segment_concat</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row30_col1\" class=\"data row30 col1\" >230</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2level0_row31\" class=\"row_heading level0 row31\" >31</th> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row31_col0\" class=\"data row31 col0\" >content_message_concat</td> \n",
       "        <td id=\"T_45493940_d617_11ea_8310_a44cc85d0dd2row31_col1\" class=\"data row31 col1\" >126</td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0xd3efc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Missing Values in Data\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot convert the series to <type 'float'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-949bbdbac176>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdate_cols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'period_week'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'period_month'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_categories\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'product'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-427eba541dde>\u001b[0m in \u001b[0;36mdescribe\u001b[1;34m(data, name, date_cols, show_categories, plot_missing, target)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Missing Values in Data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m     \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisplay_missing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m     \u001b[0m_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-d9e643eb6d4f>\u001b[0m in \u001b[0;36mdisplay_missing\u001b[1;34m(data, plot)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'features'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'missing_counts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mmissing_percent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'missing_counts'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'missing_percent'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmissing_percent\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\shari\\Anaconda2.7\\lib\\site-packages\\pandas\\core\\series.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         raise TypeError(\"cannot convert the series to \"\n\u001b[1;32m--> 117\u001b[1;33m                         \"{0}\".format(str(converter)))\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot convert the series to <type 'float'>"
     ]
    }
   ],
   "source": [
    "describe(data=data, name='', date_cols=['period_week','period_month', 'date'], show_categories=False, plot_missing=False, target='product')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped ['activity_channel', 'indication', 'therapeutic_area', 'measure_source', 'con_size']\n",
      "Dropped []\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'date_processing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-78b2d3c7bf60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'product'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdate_cols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'period_week'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'period_month'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpercent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-622de6ae864e>\u001b[0m in \u001b[0;36mfeature_processing\u001b[1;34m(data, target, date_cols, percent, method_encoding, method_scaling)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdate_cols\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mdata_dates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdate_cols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mdata_dates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdate_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_dates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdate_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdate_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[0mdata_dates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_dates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'date_processing' is not defined"
     ]
    }
   ],
   "source": [
    "data1 = feature_processing(data=data, target='product', date_cols=['period_week','period_month', 'date'], percent = 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-1f21153df4f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'product'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'product'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data1' is not defined"
     ]
    }
   ],
   "source": [
    "X = data1.drop('product', axis=1)\n",
    "y = data1['product']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_normalize_data(X, method='scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_normalize_data(X, method='normalize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_normalize_data(X, method='scale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes.\n",
    "\n",
    "1. Added New functions for scaling and normalising dataset. Then a master function 'scale_normalize_data' that use standard_scaler for normalization as default. It can also use robust_scaler when method is set to 'normalize_outlier'. It also uses minmax_scalere for scaling when method is set to'scale'. \n",
    "2. Made change to the feature_processing function to only output numeric columns as part of the dataframe. \n",
    "\n",
    "3. The 3 master functions are **describe** and **feature_processing**  functions.\n",
    "4. The hashing of the ids is done by the **feature_processing** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
