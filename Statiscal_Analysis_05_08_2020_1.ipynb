{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "L0tTLT41-soE",
    "outputId": "4495b69c-7bd3-44dd-f1f1-305e95492298"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: category_encoders in c:\\users\\ighdaro emwinghare\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in c:\\users\\ighdaro emwinghare\\anaconda3\\lib\\site-packages (from category_encoders) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\ighdaro emwinghare\\anaconda3\\lib\\site-packages (from category_encoders) (1.18.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\ighdaro emwinghare\\anaconda3\\lib\\site-packages (from category_encoders) (0.22.1)\n",
      "Requirement already satisfied: patsy>=0.5.1 in c:\\users\\ighdaro emwinghare\\anaconda3\\lib\\site-packages (from category_encoders) (0.5.1)\n",
      "Requirement already satisfied: pandas>=0.21.1 in c:\\users\\ighdaro emwinghare\\anaconda3\\lib\\site-packages (from category_encoders) (1.0.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\ighdaro emwinghare\\anaconda3\\lib\\site-packages (from category_encoders) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\ighdaro emwinghare\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->category_encoders) (0.14.1)\n",
      "Requirement already satisfied: six in c:\\users\\ighdaro emwinghare\\anaconda3\\lib\\site-packages (from patsy>=0.5.1->category_encoders) (1.14.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\ighdaro emwinghare\\anaconda3\\lib\\site-packages (from pandas>=0.21.1->category_encoders) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\ighdaro emwinghare\\anaconda3\\lib\\site-packages (from pandas>=0.21.1->category_encoders) (2.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install  category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A5pkZxCN-T2B"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This module contains all functions relating to the cleaning, exploration and feature engineering of structured data sets; mostly in pandas format\n",
    "\n",
    "\n",
    "install the anaconda python distribution to be able to use numpy, pandas, matplotlib, seaborn, scipy, sklearn and datetime\n",
    "\n",
    "use pip install category_encoders for category_encoders \n",
    "use pip install category_embedder for category_embedder. It requires the tensorflow and keras\n",
    "'''\n",
    "\n",
    "##pip install category_encoders\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from category_encoders import *\n",
    "from IPython.display import display\n",
    "from collections import Counter\n",
    "import scipy.stats as sp\n",
    "import datetime as dt\n",
    "import re\n",
    "#import categorical_embedder\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, RobustScaler, StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "#from tensorflow.keras import layers\n",
    "\n",
    "#print(tf.__version__)\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wQ39lOZb-T2q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QouN9OJc-T3D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5NEDoyct-T3V"
   },
   "source": [
    "## Exploratory and Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TCq54lGj-T3Y"
   },
   "outputs": [],
   "source": [
    "def get_cat_feats(data=None):\n",
    "    '''\n",
    "    Returns the categorical features in a data set\n",
    "    Parameters:\n",
    "    -----------\n",
    "        data: DataFrame or named Series \n",
    "    Returns:\n",
    "    -------\n",
    "        List\n",
    "            A list of all the categorical features in a dataset.\n",
    "    it is used as a helper function for most of the functions to get categorical variables\n",
    "    '''\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame or Series, got 'None'\")\n",
    "\n",
    "    cat_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "    return list(cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IvfBDvCP-T3u"
   },
   "outputs": [],
   "source": [
    "def get_num_feats(data=None):\n",
    "    '''\n",
    "    Returns the numerical features in a data set\n",
    "    Parameters:\n",
    "    -----------\n",
    "        data: DataFrame or named Series \n",
    "    Returns:\n",
    "    -------\n",
    "        List:\n",
    "            A list of all the numerical features in a dataset.\n",
    "    it is used as a helper function for most of the functions to get categorical variables\n",
    "    '''\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame or Series, got 'None'\")\n",
    "\n",
    "    num_features = data.select_dtypes(include=numerics).columns\n",
    "\n",
    "    return list(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4PqAICUN-T4K"
   },
   "outputs": [],
   "source": [
    "def get_unique_counts(data=None):\n",
    "    '''\n",
    "    Gets the unique count of categorical features in a data set.\n",
    "    Parameters\n",
    "    -----------\n",
    "        data: DataFrame or named Series \n",
    "    Returns\n",
    "    -------\n",
    "        DataFrame or Series\n",
    "            Unique value counts of the features in a dataset.\n",
    "    it is used as a helper function in the describe function to get the count of unique values in the columns \n",
    "    '''\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame or Series, got 'None'\")\n",
    "\n",
    "    features = get_cat_feats(data)\n",
    "    temp_len = []\n",
    "\n",
    "    for feature in features:\n",
    "        temp_len.append(len(data[feature].unique()))\n",
    "        \n",
    "    df = list(zip(features, temp_len))\n",
    "    df = pd.DataFrame(df, columns=['Feature', 'Unique Count'])\n",
    "    df = df.style.bar(subset=['Unique Count'], align='mid')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fbOa1WmM-T4h"
   },
   "outputs": [],
   "source": [
    "def display_missing(data=None, plot=False):\n",
    "    '''\n",
    "    Display missing values as a pandas dataframe.\n",
    "    Parameters\n",
    "    ----------\n",
    "        data: DataFrame or named Series\n",
    "        plot: bool, Default False\n",
    "            Plots missing values in dataset as a heatmap\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        Matplotlib Figure:\n",
    "            Heatmap plot of missing values\n",
    "    '''\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame or Series, got 'None'\")\n",
    "\n",
    "    df = data.isna().sum()\n",
    "    df = df.reset_index()\n",
    "    df.columns = ['features', 'missing_counts']\n",
    "\n",
    "    missing_percent = round((df['missing_counts'] / data.shape[0]) * 100, 1)\n",
    "    df['missing_percent'] = missing_percent\n",
    "\n",
    "    if plot:\n",
    "        plot_missing(data)\n",
    "        return df\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CMxCs7dQ-T43"
   },
   "outputs": [],
   "source": [
    "def cat_summarizer(data, x=None, y=None, hue=None, palette='Set1', verbose=True):\n",
    "    '''\n",
    "    Helper function that gives a quick summary of a given column of categorical data\n",
    "    Parameters:\n",
    "    ---------------------------\n",
    "        dataframe: pandas dataframe\n",
    "        x: str.\n",
    "            horizontal axis to plot the labels of categorical data, y would be the count.\n",
    "        y: str. \n",
    "            vertical axis to plot the labels of categorical data, x would be the count.\n",
    "        hue: str. i\n",
    "            if you want to compare it another variable (usually the target variable)\n",
    "        palette: array, list.\n",
    "            Colour of the plot\n",
    "    Returns:\n",
    "    ----------------------\n",
    "        Quick Stats of the data and also the count plot\n",
    "        \n",
    "        it is used in the describe function\n",
    "    '''\n",
    "    if x == None:\n",
    "        column_interested = y\n",
    "    else:\n",
    "        column_interested = x\n",
    "    series = data[column_interested]\n",
    "    print(series.describe())\n",
    "    print('mode: ', series.mode())\n",
    "    if verbose:\n",
    "        print('='*80)\n",
    "        print(series.value_counts())\n",
    "\n",
    "    sns.countplot(x=x, y=y, hue=hue, data=data, palette=palette)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wO21LwI_-T5a"
   },
   "outputs": [],
   "source": [
    "def _space():\n",
    "    '''it is used in  most functions to add space. this makes result more presentation'''\n",
    "    print('\\n')\n",
    "def _match_date(data):\n",
    "    '''\n",
    "        Return a list of columns that matches the DateTime expression\n",
    "    '''\n",
    "    mask = data.sample(20).astype(str).apply(lambda x : x.str.match(r'(\\d{2,4}-\\d{2}-\\d{2,4})+').all())\n",
    "    return set(data.loc[:, mask].columns)\n",
    "\n",
    "\n",
    "def display_rows(data,num=2):\n",
    "    '''\n",
    "    Displays the required number of rows\n",
    "    it is used in the describe function\n",
    "    '''\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame/ numpy2d array, got 'None'\")\n",
    "\n",
    "    return data.head(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZGR27Yn--T5p"
   },
   "outputs": [],
   "source": [
    "def plot_missing(data=None):\n",
    "    '''\n",
    "    Plots the data as a heatmap to show missing values\n",
    "    Parameters\n",
    "    ----------\n",
    "        data: DataFrame, array, or list of arrays.\n",
    "            The data to plot.\n",
    "    '''\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame or Series, got 'None'\")\n",
    "\n",
    "    sns.heatmap(data.isnull(), cbar=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3myfo3U3-T54"
   },
   "outputs": [],
   "source": [
    "def class_count(data=None, features=None, plot=False, save_fig=False):\n",
    "    '''df, method='up_sample_minority_class',target,majority_class= 0,minority_class= 1,number_of_samples= 3000\n",
    "    Displays the number of classes in a categorical feature.\n",
    "    Parameters:\n",
    "    \n",
    "        data: Pandas DataFrame or Series\n",
    "            Dataset for plotting.\n",
    "        features: Scalar, array, or list. \n",
    "            The categorical features in the dataset, if None, \n",
    "            we try to infer the categorical columns from the dataframe.\n",
    "        plot: bool, Default False.\n",
    "            Plots the class counts as a barplot\n",
    "        save_fig: bool, Default False.\n",
    "            Saves the plot to the current working directory.\n",
    "    it is used in the describe function\n",
    "    '''\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame or Series, got 'None'\")\n",
    "\n",
    "    if features is None:\n",
    "        features = get_cat_feats(data)\n",
    "\n",
    "                        \n",
    "\n",
    "    for feature in features:\n",
    "        if data[feature].nunique() > 15:\n",
    "            print(\"Unique classes in {} too large\".format(feature))\n",
    "        else:\n",
    "            print('Class Count for', feature)\n",
    "            display(pd.DataFrame(data[feature].value_counts()))\n",
    "\n",
    "    if plot:\n",
    "        countplot(data, features, save_fig=save_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ypMmLSh1-T6V"
   },
   "outputs": [],
   "source": [
    "def get_date_cols(data=None):\n",
    "    '''\n",
    "    Returns the Datetime columns in a data set.\n",
    "    Parameters\n",
    "    ----------\n",
    "        data: DataFrame or named Series\n",
    "            Data set to infer datetime columns from.\n",
    "        convert: bool, Default True\n",
    "            Converts the inferred date columns to pandas DateTime type\n",
    "    Returns:\n",
    "    -------\n",
    "        List\n",
    "         Date column names in the data set\n",
    "    use in the describe function to set date columns to datetime datatype in utc\n",
    "    '''\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame or Series, got 'None'\")\n",
    "\n",
    "    #Get existing date columns in pandas Datetime64 format\n",
    "    date_cols = set(data.dtypes[data.dtypes == 'datetime64[ns, UTC]'].index)\n",
    "    #infer Date columns \n",
    "    date_cols = date_cols.union(_match_date(data))\n",
    "       \n",
    "    return date_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sVYIvBCQ-T6y"
   },
   "outputs": [],
   "source": [
    "def bivariate_stats(data):\n",
    "    '''Returns the contingency table and chi2 contingency test result between columns in the dataframe\n",
    "        \n",
    "        it is used in the describe function for categorical features analysis \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    cat_feats = get_cat_feats(data=data)\n",
    "    counter = 1\n",
    "    try:\n",
    "        while counter<(len(cat_feats)):\n",
    "            val1 = get_cat_feats(data=datainput)[counter - 1]\n",
    "            val2 = get_cat_feats(data=datainput)[counter]\n",
    "            if (data[val1].nunique() > 15) or (data[val2].nunique() > 15):\n",
    "                print('Number of unique values too large')\n",
    "            else:\n",
    "                freqtab = pd.crosstab(data[val1], data[val2])\n",
    "                print(\"Frequency table\")\n",
    "                print(\"============================\")\n",
    "                print(freqtab)\n",
    "                print(\"============================\")\n",
    "                chi2, pval, dof, expected = sp.chi2_contingency(freqtab)\n",
    "                print(\"ChiSquare test statistic: \",chi2)\n",
    "                print(\"p-value: \",pval)\n",
    "                _space()\n",
    "            counter= counter+1\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pFXnoheN-T7H"
   },
   "outputs": [],
   "source": [
    "\n",
    "def bivariate_stats_target(data, target):\n",
    "    \n",
    "    '''Returns the contingency table and chi2 contingency test result between columns and the target variable in the dataframe\n",
    "        \n",
    "        \n",
    "        Parameters\n",
    "    ----------\n",
    "        data: DataFrame or named Series\n",
    "            Data set to infer datetime columns from.\n",
    "        target: the target variable in form of string\n",
    "\n",
    "        \n",
    "        it is used in the describe function for categorical features analysis of the relationship between the target variable \n",
    "        and other categorical features\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    cat_feats = get_cat_feats(data=data)\n",
    "    for i in cat_feats:\n",
    "        if (data[i].nunique() > 20):\n",
    "            print('Number of Unique values too large')\n",
    "        else:\n",
    "            freqtab = pd.crosstab(data[i], data[target])\n",
    "            print(\"Frequency table\")\n",
    "            print(\"============================\")\n",
    "            print(freqtab)\n",
    "            print(\"============================\")\n",
    "            chi2, pval, dof, expected = sp.chi2_contingency(freqtab)\n",
    "            print(\"ChiSquare test statistic: \",chi2)\n",
    "            print(\"p-value: \",pval)\n",
    "            _space()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BKcqKKPh-T7Z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m_wtoPSd-T7-"
   },
   "source": [
    "## Feature Processing; Missing Value Imputation, Date Feature Extraction and Categorical Feature Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Y-7NAaL-T8B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_YsraMlv-T8u"
   },
   "outputs": [],
   "source": [
    "def date_processing(data=None, date_cols = None, utc=True):\n",
    "    '''\n",
    "    This function is used for preprocessing date columns and creating new columns from the date columns.\n",
    "    \n",
    "    It is used in the feature_processing function.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame/ numpy2d array, got 'None'\")\n",
    "        \n",
    "    for i in date_cols:\n",
    "        data[i] = pd.to_datetime(data[i], utc=utc)\n",
    "        data[f'month_{i}'] = data[i].dt.month\n",
    "        data[f'year_{i}'] = data[i].dt.year\n",
    "        data[f'day_{i}'] = data[i].dt.day\n",
    "        data[f'dayweek_{i}'] = data[i].dt.weekday\n",
    "        #data = data.drop(date_cols, axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vdn-fLDb-T_i"
   },
   "outputs": [],
   "source": [
    "def drop_missing(data=None, percent=99):\n",
    "    '''\n",
    "    Drops missing columns with [percent] of missing data.\n",
    "    Parameters:\n",
    "    -------------------------\n",
    "        data: Pandas DataFrame or Series.\n",
    "        percent: float, Default 99\n",
    "            Percentage of missing values to be in a column before it is eligible for removal.\n",
    "    Returns:\n",
    "    ------------------\n",
    "        Pandas DataFrame or Series.\n",
    "    It can be used alone. It also used in deal_with_missing_value function.\n",
    "    \n",
    "    This function is used in the deal_with_missing_value function.\n",
    "    '''\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame/ numpy2d array, got 'None'\")\n",
    "    \n",
    "    missing_percent = (data.isna().sum() / data.shape[0]) * 100\n",
    "    cols_2_drop = missing_percent[missing_percent.values >= percent].index\n",
    "    print(\"Dropped {}\".format(list(cols_2_drop)))\n",
    "    #Drop missing values\n",
    "    df = data.drop(cols_2_drop, axis=1)\n",
    "    return df\n",
    "\n",
    "def fill_missing_cats(data=None, cat_features=None, missing_encoding=None, missing_col=False):\n",
    "    '''\n",
    "    Fill missing values using the mode of the categorical features.\n",
    "    Parameters:\n",
    "    ------------------------\n",
    "        data: DataFrame or name Series.\n",
    "            Data set to perform operation on.\n",
    "        cat_features: List, Series, Array.\n",
    "            categorical features to perform operation on. If not provided, we automatically infer the categoricals from the dataset.\n",
    "        missing_encoding: List, Series, Array.\n",
    "            Values used in place of missing. Popular formats are [-1, -999, -99, '', ' ']\n",
    "        missin_col: bool, Default True\n",
    "      Creates a new column to capture the missing values. 1 if missing and 0 otherwise. This can sometimes help a machine learning model.\n",
    "      \n",
    "      This function is used in the deal_with_missing_value function.\n",
    "    '''\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame/ numpy2d array, got 'None'\")\n",
    "\n",
    "    if cat_features is None:\n",
    "        cat_features = get_cat_feats(data)\n",
    "\n",
    "    df = data.copy()\n",
    "    #change all possible missing values to NaN\n",
    "    if missing_encoding is None:\n",
    "        missing_encoding = ['', ' ', -99, -999]\n",
    "\n",
    "    df.replace(missing_encoding, np.NaN, inplace=True)\n",
    "    \n",
    "    for feat in cat_features:\n",
    "        if missing_col:\n",
    "            df[feat + '_missing_value'] = (df[feat].isna()).astype('int64')\n",
    "        most_freq = df[feat].mode()[0]\n",
    "        df[feat] = df[feat].replace(np.NaN, most_freq)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def fill_missing_num(data=None, num_features=None, method='mean', missing_col=False):\n",
    "    '''\n",
    "    fill missing values in numerical columns with specified [method] value\n",
    "    Parameters:\n",
    "        ------------------------------\n",
    "        data: DataFrame or name Series.\n",
    "            The data set to fill\n",
    "        features: list.\n",
    "            List of columns to fill\n",
    "        method: str, Default 'mean'.\n",
    "            method to use in calculating fill value.\n",
    "        missing_col: bool, Default True\n",
    "          Creates a new column to capture the missing values. 1 if missing and 0 otherwise. This can sometimes help a machine learning model.\n",
    "          \n",
    "          This function is used in the deal_with_missing_value function.\n",
    "    '''\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame/ numpy2d array, got 'None'\")\n",
    "    \n",
    "    if num_features is None:\n",
    "        num_features = get_num_feats(data)\n",
    "        #get numerical features with missing values\n",
    "        temp_df = data[num_features].isna().sum()\n",
    "        features = list(temp_df[num_features][temp_df[num_features] > 0].index)\n",
    "        \n",
    "    df = data.copy()\n",
    "    for feat in features:\n",
    "        if missing_col:\n",
    "            df[feat + '_missing_value'] = (df[feat].isna()).astype('int64')\n",
    "        if method is 'mean':\n",
    "            mean = df[feat].mean()\n",
    "            df[feat].fillna(mean, inplace=True)\n",
    "        elif method is 'median':\n",
    "            median = df[feat].median()\n",
    "            df[feat].fillna(median, inplace=True)\n",
    "        elif method is 'mode':\n",
    "            mode = df[feat].mode()[0]\n",
    "            df[feat].fillna(mode, inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"method: must specify a fill method, one of [mean, mode or median]'\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mokb1psS-T_4"
   },
   "outputs": [],
   "source": [
    "def deal_with_missing_value(data, percent=70):\n",
    "    \"\"\"\n",
    "    this function automatically take care of missing values.\n",
    "        It fills the missing values in categorical variables with mode of the particular column\n",
    "        and fills the missing value numerical variables with mean of the particular column.\n",
    "        It automatically drops columns with more than 70% missing values except when set otherwise.\n",
    "        \n",
    "        This function is used in the feature_preprocessing function to deal with missing values. It can also be used alone.\n",
    "        \"\"\"\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame/ numpy2d array, got 'None'\")\n",
    "    df1 = drop_missing(data=data, percent=percent)\n",
    "    df2 = fill_missing_cats(data=df1)\n",
    "    df = fill_missing_num(data=df2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHh7cjBm-UAZ"
   },
   "outputs": [],
   "source": [
    "def drop_redundant(data):\n",
    "    '''\n",
    "    Removes features with the same value in all cell. Drops feature If Nan is the second unique class as well.\n",
    "    Parameters:\n",
    "    -----------------------------\n",
    "        data: DataFrame or named series.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame or named series.\n",
    "    This function is used in the feature_processing function.\n",
    "    '''\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame/ numpy2d array, got 'None'\")\n",
    "    \n",
    "    #get columns\n",
    "    cols_2_drop = _nan_in_class(data)\n",
    "    print(\"Dropped {}\".format(cols_2_drop))\n",
    "    df = data.drop(cols_2_drop, axis=1)\n",
    "    return df\n",
    "def _nan_in_class(data):\n",
    "    \"\"\"helper function for drop_redundant function\"\"\"\n",
    "    cols = []\n",
    "    for col in data.columns:\n",
    "        if len(data[col].unique()) == 1:\n",
    "            cols.append(col)\n",
    "\n",
    "        if len(data[col].unique()) == 2:\n",
    "            if np.nan in list(data[col].unique()):\n",
    "                cols.append(col)\n",
    "\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vlKkqUYV-UBh"
   },
   "source": [
    "### Different Encoding Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iVuTnJcK-UBk"
   },
   "outputs": [],
   "source": [
    "#Label Encoding for object to numeric conversion\n",
    "def binaryencoder(data):\n",
    "    \"\"\"To avoid the curse of dimensionality, this function only encodes categorical features with less than 7 unique values\n",
    "    \n",
    "        This function can be used alone. It is also used in the encode_data function.\n",
    "        \n",
    "        It is the default method for encoding categorical variables with unique value less than four in the encode_data function.\n",
    "        \n",
    "        explanation: It is similar to onehot encoding but gives lesser dimensions, making it a better option. \n",
    "                    It converts the unique entry into binary combination and then creates column using binary hashing.\n",
    "    \"\"\"\n",
    "    features = get_cat_feats(data=data)\n",
    "    cols = []\n",
    "    for feature in features:\n",
    "        if data[feature].nunique() < 7:\n",
    "            cols.append(feature)\n",
    "        \n",
    "    enc = BinaryEncoder(cols=cols).fit(data)\n",
    "    data = enc.transform(data)\n",
    "    return data\n",
    "\n",
    "def onehotencoder(data):\n",
    "    \"\"\"To avoid the curse of dimensionality, this function only encodes categorical features with less than 4 unique values\n",
    "    \n",
    "        This function can be used alone. It is also used in the encode_data function\n",
    "        \n",
    "    The onehotencoder is only used when the number of unique value is less than four to avoid the curse of dimensionality.\n",
    "    If encode_data method parameter is set to 'onehotencode' this is what is used in encoding categorical variables with \n",
    "    number of unique values less than 4.\n",
    "    \n",
    "    explanation: it is used mostly for nominal variables such that a binary combination of the unique values are set as new columns\n",
    "                    in the dataset.\n",
    "    \"\"\"\n",
    "    features = get_cat_feats(data=data)\n",
    "    cols = []\n",
    "    for feature in features:\n",
    "        if data[feature].nunique() < 4:\n",
    "            cols.append(feature)\n",
    "        \n",
    "    enc = OneHotEncoder(cols=cols).fit(data)\n",
    "    data = enc.transform(data)\n",
    "    return data\n",
    "\n",
    "def labelencoder(data):\n",
    "    \"\"\"\n",
    "    This function can be used alone. It is also used in the encode_data function.\n",
    "    \n",
    "    It is used for columns that has more than 3 unique values. Such columns are treated as ordinal variables. \n",
    "    \n",
    "    Explanation: Label encoders are ordinal encoders that encode unique values as continuous intergers.\n",
    "    \n",
    "    \"\"\"\n",
    "    features = get_cat_feats(data=data)\n",
    "    for feat in features:\n",
    "        data[feat] = le.fit_transform(data[feat].astype(str))\n",
    "    return data\n",
    "\n",
    "def sumencoder(data):\n",
    "    \"\"\"\n",
    "    This function can be used alone. It is also used in the encode_data function.\n",
    "    \n",
    "    The sumencoder is only used when the number of unique value is less than four to avoid the curse of dimensionality.\n",
    "    If encode_data method parameter is set to 'sumencode' this is what is used in encoding categorical variables with \n",
    "    number of unique values less than 4.\n",
    "    \n",
    "    explanation: it is similar to one-hot encoding but the difference is that in sum encoding one value is taken as '-1'\n",
    "                and it is not compared to other value.\n",
    "    \"\"\"\n",
    "\n",
    "    features = get_cat_feats(data=data)\n",
    "    cols = []\n",
    "    for feature in features:\n",
    "        if data[feature].nunique() < 4:\n",
    "            cols.append(feature)\n",
    "    enc = SumEncoder(cols = cols).fit_transform(data)\n",
    "    data = enc\n",
    "    return data\n",
    "\n",
    "def catboostencoder(data, target):\n",
    "    '''Data inputs must not be string\n",
    "        This function is used alone. It is not called by any other function.\n",
    "        \n",
    "        it uses the catboost tree model in properly encoding categorical features.\n",
    "    \n",
    "        explanation: a target encoder. It uses the target variable in encoding the categorical variables. \n",
    "        It is more accurate than most encoding methods.\n",
    "    '''\n",
    "    X = data.drop(target, axis=1)\n",
    "    y = data[target]\n",
    "    features = get_cat_feats(data=X)\n",
    "    enc = CatBoostEncoder(cols=features).fit(X,y)\n",
    "    data = enc.transform(X, y)\n",
    "    return data\n",
    "\n",
    "def hashencoder(data):\n",
    "    '''\n",
    "         This function can be used alone. It is also used in the encode_data function.\n",
    "    \n",
    "        The hashencoder is only used when the number of unique value is less than four to avoid the curse of dimensionality.\n",
    "        If encode_data method parameter is set to 'hashencode' this is what is used in encoding categorical variables with \n",
    "        number of unique values less than 4.\n",
    "\n",
    "        explanation: Feature hashing maps each category in a categorical feature to an integer within a predetermined range\n",
    "    \n",
    "                        The size of the output dimensions is controlled by the variable n_components.\n",
    "    '''    \n",
    "    \n",
    "    cols = get_cat_feats(data)\n",
    "    new_col = []\n",
    "    for i in cols:\n",
    "        string = str(data[i][0]) + str(data[i][len(data)-1]) \n",
    "        flag = re.findall(r'\\d+', string)\n",
    "        if len(flag) > 2:\n",
    "               if len(flag[0])>2:\n",
    "                    new_col.append(i)\n",
    "        \n",
    "    enc = HashingEncoder(cols=new_col, n_components= 1).fit(data)\n",
    "    data = enc.transform(data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def embeddingencoder(data):\n",
    "     \"\"\"\n",
    "        This function is used alone.\n",
    "        It uses neural network embeddings to encode categorical features. \n",
    "         \"\"\"\n",
    "     embedding_info = ce.get_embedding_info(data)\n",
    "     X_encoded,encoders = ce.get_label_encoded_data(data)\n",
    "\n",
    "     return X_endoded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VviXI2rD-UCN"
   },
   "outputs": [],
   "source": [
    "def encode_data(data, method='binary'):\n",
    "    \"\"\"\n",
    "        encodes categorical variables automatically using binary encoding for columns with less than 4 unique values\n",
    "        then label encode all other variables\n",
    "        method takes either binary or onehot or sumencode or hashcode. default is binary\n",
    "        \n",
    "        this function can be used alone and it also used in the feature_processing function.\n",
    "    \n",
    "    \"\"\"\n",
    "    if method == 'binary':\n",
    "        data = binaryencoder(data)\n",
    "    elif method== 'onehot':\n",
    "        data = onehotencoder(data)\n",
    "    elif method == 'sumencode':\n",
    "        data = sumencode(data)\n",
    "    data = labelencoder(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AIn2Fta2-UCw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hFzipKEI-UDS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n2miK9yK-UDw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PjtKYQr7-UEK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v1GP-8sx-UEw"
   },
   "outputs": [],
   "source": [
    "def get_qcut(data=None, col=None, q=None, duplicates='drop', return_type='float64'):\n",
    "    '''\n",
    "    Cuts a series into bins using the pandas qcut function\n",
    "    and returns the resulting bins as a series for merging.\n",
    "    Parameter:\n",
    "    -------------\n",
    "        data: DataFrame, named Series\n",
    "            Data set to perform operation on.\n",
    "        col: str\n",
    "            column to cut/binnarize.\n",
    "        q: integer or array of quantiles\n",
    "            Number of quantiles. 10 for deciles, 4 for quartiles, etc. Alternately array of quantiles, e.g. [0, .25, .5, .75, 1.] for quartiles.\n",
    "        duplicates: Default 'drop',\n",
    "            If bin edges are not unique drop non-uniques.\n",
    "        return_type: dtype, Default (float64)\n",
    "            Dtype of series to return. One of [float64, str, int64]\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "        Series, 1D-Array\n",
    "        \n",
    "        This function is not used in any other function. It can be used alone.\n",
    "    '''\n",
    "\n",
    "    temp_df = pd.qcut(data[col], q=q, duplicates=duplicates).to_frame().astype('str')\n",
    "    #retrieve only the qcut categories\n",
    "    df = temp_df[col].str.split(',').apply(lambda x: x[0][1:]).astype(return_type)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xyD6RGRL-UFB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zUMwDKvI-UFP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LRFAwvcn-UFb"
   },
   "source": [
    "## Scaling and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KbrjDnyg_lwP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DwLjlz-L-UFd",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def minmaxscaler(data, cols = None):\n",
    "    '''\n",
    "    It is scaling method use when you want all the values in column to be between 0 and 1. This is used for parametric models\n",
    "    like SVM and KNN that calculate euclidean distance or any other form of distance using the magnitude of the values.\n",
    "    \n",
    "    -------------\n",
    "        data: DataFrame, named Series\n",
    "            Data set to perform operation on. It advisable not to scale/normalise the target variable.\n",
    "        col: list of str\n",
    "            columns in form of a list to scale/normalise. If not parsed, it scales/normalises the entire dataframe\n",
    "        Returns:\n",
    "    --------\n",
    "        DataFrame of the scaled/normalised data/columns.\n",
    "    It is used in the scale_normalise_data. It can also be used alone.\n",
    "    '''\n",
    "    \n",
    "    if cols is not None:\n",
    "        mm_scaler = MinMaxScaler()\n",
    "        data[cols] = mm_scaler.fit_transform(data[cols])\n",
    "       \n",
    "    else:\n",
    "        col_names = data.columns\n",
    "        mm_scaler = MinMaxScaler()\n",
    "        df_mm = mm_scaler.fit_transform(data)\n",
    "        data = pd.DataFrame(df_mm, columns=col_names)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WPaH_0fQ-UFs"
   },
   "outputs": [],
   "source": [
    "def standardscaler(data, cols = None):\n",
    "    ''' \n",
    "    It is a standard normalization technique use when using machine learning models that assusme a normal/gaussian distribution.\n",
    "    Models like Linear Regression, Gaussian Naive Bayes etc.\n",
    "    -------------\n",
    "        data: DataFrame, named Series\n",
    "            Data set to perform operation on. It advisable not to scale/normalise the target variable.\n",
    "        col: list of str\n",
    "            columns in form of a list to scale/normalise. If not parsed, it scales/normalises the entire dataframe\n",
    "        Returns:\n",
    "    --------\n",
    "        DataFrame of the scaled/normalised data/columns.\n",
    "        \n",
    "    It is used in the scale_normalise_data. It can also be used alone.\n",
    "     '''\n",
    "    if cols is not None:\n",
    "        \n",
    "        s_scaler = StandardScaler()\n",
    "        data[cols] = s_scaler.fit_transform(data[cols])\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        col_names = data.columns\n",
    "        s_scaler = StandardScaler()\n",
    "        df_s = s_scaler.fit_transform(data)\n",
    "        data = pd.DataFrame(df_s, columns=col_names)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2HJ3UmoS-UF5"
   },
   "outputs": [],
   "source": [
    "def robustscaler(data, cols = None):\n",
    "    \n",
    "    '''\n",
    "    It is similar to the standardscaler, except that is is better used when there are outliers in the dataset.\n",
    "    -------------\n",
    "        data: DataFrame, named Series\n",
    "            Data set to perform operation on. It advisable not to scale/normalise the target variable.\n",
    "        col: list of str\n",
    "            columns in form of a list to scale/normalise. If not parsed, it scales/normalises the entire dataframe\n",
    "        Returns:\n",
    "    --------\n",
    "        DataFrame of the scaled/normalised data/columns.\n",
    "    \n",
    "    It is used in the scale_normalise_data. It can also be used alone.\n",
    "        '''\n",
    "    \n",
    "    if cols is not None:\n",
    "        r_scaler = RobustScaler()\n",
    "        data[cols] = r_scaler.fit_transform(data[cols])\n",
    "    else:\n",
    "        col_names = data.columns\n",
    "        r_scaler = RobustScaler()\n",
    "        df_r = r_scaler.fit_transform(data)\n",
    "        data = pd.DataFrame(df_r, columns=col_names)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a94YAH1i-UGI"
   },
   "outputs": [],
   "source": [
    "def conventional_feat_scaling(data, cols = None, method = 'log', interval=[0,1]):\n",
    "    \n",
    "    \"\"\"\n",
    "    It uses conventional methodz for scaling variables.\n",
    "    -------------\n",
    "        data: DataFrame, named Series\n",
    "            Data set to perform operation on. It advisable not to scale/normalise the target variable.\n",
    "        col: list of str\n",
    "            columns in form of a list to scale/normalise. If cols is not parsed, it won't work.\n",
    "        method:str\n",
    "            the method of scaling to be used. It can be 'log','linear', 'clipping' or 'z_score'\n",
    "        interval: list of int\n",
    "            this is used by the clipping scaling method. the first element is the min val and the second element is the mex val.\n",
    "        Returns:\n",
    "    --------\n",
    "        DataFrame of the scaled/normalised data.\n",
    "    It is used alone.\n",
    "    \"\"\"\n",
    "    if cols is not None:\n",
    "        for i in cols:\n",
    "            if method == 'log':\n",
    "                data[i] = np.log((1+data[i])/2)\n",
    "            if method == 'linear':\n",
    "                maximum = np.max(data[i])\n",
    "                data[i] = data[i]/maximum\n",
    "            if method == 'clipping':\n",
    "                data[i] = np.clip(data[i], interval[0], interval[1])\n",
    "            if method == 'z_score':\n",
    "                data[i] = (data[i] - np.mean(data[i]))/np.std(data[i])\n",
    "                \n",
    "    else: \n",
    "        print(\"Input the cols parameter. This function works on the columns parsed in the cols parameter\")\n",
    "    \n",
    "    return data\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VMV50LCp-UGV"
   },
   "outputs": [],
   "source": [
    "def scale_normalize_data(data, cols = None, method = 'normalize'):\n",
    "    '''Normalises or Scale variables in the dataset. \n",
    "        NB:1. Normalise all features if the ml model assumes a normal/gaussian distribution.\n",
    "                Example of such models are linear regression, discriminant analysis (LDA) and Gaussian naive Bayes.\n",
    "                If statistical test such as t-test, ANOVA and other tests that assume normal distribution is to be carried out, \n",
    "                then normalise the data before carryingout the test.\n",
    "            2. Scale the data if to be in range of -1 and 1 \n",
    "                if parametric model like K Nearest Neighbour and Support Vector Machines are to be used.\n",
    "        \n",
    "        -------------\n",
    "        data: DataFrame, named Series\n",
    "            Data set to perform operation on. It advisable not to scale/normalise the target variable.\n",
    "        col: list of str\n",
    "            columns in form of a list to scale/normalise. If not parsed, it scales/normalises the entire dataframe\n",
    "        method: str\n",
    "            The method can either be normalize, normalize_outlier, or scale. It differes base on the used case. The default is normalise.\n",
    "        return_type: DataFrame\n",
    "            \n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "        DataFrame of the scaled/normalised data/columns.\n",
    "        \n",
    "    This function is not used in any other function. It can be used alone.\n",
    "    '''\n",
    "    data = data[get_num_feats(data=data)]\n",
    "    if method == 'normalize':\n",
    "        df = standardscaler(data, cols = cols)\n",
    "    elif method == 'normalize_outlier':\n",
    "        df = robustscaler(data, cols = cols)\n",
    "    elif method == 'scale':\n",
    "        df = minmaxscaler(data, cols = cols)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TEAFOJV--UGi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jrrVCaMEOK1E"
   },
   "source": [
    "## visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bRzbR-hn_wN_"
   },
   "outputs": [],
   "source": [
    "def heatmap(data):\n",
    "    '''\n",
    "    shows heatmap of missing values\n",
    "\n",
    "    '''\n",
    "    sns.heatmap(data.isnull(), cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ifH1O5h_wJl"
   },
   "outputs": [],
   "source": [
    "def count_check(data):\n",
    "    ''' \n",
    "    unique count check\n",
    "\n",
    "    '''\n",
    "    feats = [f for f in data.columns]\n",
    "    for i in feats:\n",
    "        print ('==' + str(i) + '==')\n",
    "        print ('data:' + str(data[i].nunique()/data.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l_Dt5VdA_v_6"
   },
   "outputs": [],
   "source": [
    "def mean_std(data):\n",
    "    # Distribution of mean and std\n",
    "    feats  = [f for f in data.columns ]\n",
    "    plt.figure(figsize=(16,6))\n",
    "    features = data[feats].columns.values\n",
    "    plt.title(\"Distribution of mean values per row in the data\")\n",
    "    sns.distplot(data[features].mean(axis=1),color=\"green\", kde=True,bins=120, label='data')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G8cYGYYEDcNo"
   },
   "outputs": [],
   "source": [
    "def  cat_plot(data):\n",
    "    '''plot multiple categorical features as bar charts on the same plot\n",
    "\n",
    "    '''\n",
    "    feats  = [f for f in data.columns ]\n",
    "    categorical_features = data.select_dtypes(exclude=[\"number\",\"bool_\"]).columns\n",
    "    fig, ax = plt.subplots(1, len(categorical_features))\n",
    "    for i, categorical_feature in enumerate(data[categorical_features]):\n",
    "        data[categorical_feature].value_counts().plot(kind =\"bar\", ax=ax[i]).set_title(categorical_feature)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_1k2TPYwDcJG"
   },
   "outputs": [],
   "source": [
    "def corr(data):\n",
    "    '''Feature Correlation\n",
    "\n",
    "    '''\n",
    "    feats  = [f for f in data.columns ]\n",
    "    correlations = data[feats].corr().unstack().sort_values(kind=\"quicksort\").reset_index()\n",
    "    correlations = correlations[correlations['level_0'] != correlations['level_1']]\n",
    "    correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZt49rD7MhsY"
   },
   "outputs": [],
   "source": [
    "def pairplot(data):\n",
    "    '''\n",
    "    pairplot on datasets\n",
    "    '''\n",
    "    sns.set(style=\"ticks\", color_codes=True)\n",
    "    g = sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "94d0dZ1iMhmW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CWuMH19vMhaI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "REkllHvG-UGv"
   },
   "source": [
    "# Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8HzVXxY8-UG1"
   },
   "outputs": [],
   "source": [
    "def describe(data=None, name='', date_cols=None, show_categories=False, plot_missing=False, target = None):\n",
    "    '''\n",
    "    Calculates statistics and information about a data set. Information displayed are\n",
    "    shapes, size, number of categorical/numeric/date features, missing values,\n",
    "    dtypes of objects etc.\n",
    "    Parameters:\n",
    "    --------------------\n",
    "        data: Pandas DataFrame\n",
    "            The data to describe.\n",
    "        name: str, optional\n",
    "            The name of the data set passed to the function.\n",
    "        date_cols: list/series/array\n",
    "            Date column names in the data set.\n",
    "        show_categories: bool, default False\n",
    "            Displays the unique classes and counts in each of the categorical feature in the data set.\n",
    "        plot_missing: bool, default True\n",
    "            Plots missing values as a heatmap\n",
    "        target: the target variable in the dataframe\n",
    "    Returns:\n",
    "    -------\n",
    "        None\n",
    "        \n",
    "        This function is stand alone use for quick statistical exploration of the data.\n",
    "    '''\n",
    "    \n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame or Series, got 'None'\")\n",
    "\n",
    "    ## Get categorical features\n",
    "    cat_features = get_cat_feats(data)\n",
    "    \n",
    "    #Get numerical features\n",
    "    num_features = get_num_feats(data)\n",
    "    try:\n",
    "        print(\"heat  map\")\n",
    "        heat   = heatmap(data)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        print(\"count checker\")\n",
    "        count_check(data)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        print(\"cat  plots\")\n",
    "        cat_plot(data)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        print(\"mean and std\")\n",
    "        mean_std(data)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        corr(data)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        pairplot(data)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    print('First five data points')\n",
    "    display(data.head())\n",
    "    _space()\n",
    "\n",
    "    print('Random five data points')\n",
    "    display(data.sample(5))\n",
    "    _space()\n",
    "\n",
    "    print('Last five data points')\n",
    "    display(data.tail())\n",
    "    _space()\n",
    "\n",
    "    print('Shape of {} data set: {}'.format(name, data.shape))\n",
    "    _space()\n",
    "\n",
    "    print('Size of {} data set: {}'.format(name, data.size))\n",
    "    _space()\n",
    "\n",
    "    print('Data Types')\n",
    "    print(\"Note: All Non-numerical features are identified as objects in pandas\")\n",
    "    display(pd.DataFrame(data.dtypes, columns=['Data Type']))\n",
    "    _space()\n",
    "    \n",
    "    date_cols = get_date_cols(data)\n",
    "    if len(date_cols) is not 0:\n",
    "        print(\"Column(s) {} should be in Datetime format. Use the [to_date] function to convert to Pandas Datetime format\".format(date_cols))\n",
    "        _space()\n",
    "\n",
    "    print('Numerical Features in Data set')\n",
    "    print(num_features)\n",
    "    _space()\n",
    "\n",
    "    print('Categorical Features in Data set')\n",
    "    display(cat_features)\n",
    "    _space()\n",
    "\n",
    "    print('Statistical Description of Columns')\n",
    "    display(data.describe())\n",
    "    _space()\n",
    "    \n",
    "    print('Description of Categorical Features')\n",
    "    if cat_features != None:\n",
    "        display(data.describe(include=[np.object, pd.Categorical]).T)\n",
    "        _space()\n",
    "          \n",
    "    print('Unique class Count of Categorical features')\n",
    "    display(get_unique_counts(data))\n",
    "    _space()\n",
    "\n",
    "    if show_categories:     \n",
    "        print('Classes in Categorical Columns')\n",
    "        print(\"-\"*30)\n",
    "        class_count(data, cat_features)\n",
    "        _space()\n",
    "\n",
    "    print('Missing Values in Data')\n",
    "    display(display_missing(data))\n",
    "    _space()\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_processing(data=None, target=None, date_cols= None, percent = 70, method_sampling  = \"up_sample_minority_class\",method_encoding ='binary', method_scaling = 'scale'):\n",
    "    '''this function does feature engineering such as dealing with missing values, dropping redundant columns,\n",
    "        encoding categorical variables, creating new variables from the date columns, gives contigency_test, \n",
    "        print out the correlation between the encoded variables and the target variable.\n",
    "        It also scales the data using minmaxscaler and handles imbalance dataset.\n",
    "        \n",
    "        Parameters\n",
    "    ----------\n",
    "        date_cols: list/series/array\n",
    "            Date column names in the data set.\n",
    "        show_categories: bool, default False\n",
    "            Displays the unique classes and counts in each of the categorical feature in the data set.\n",
    "        plot_missing: bool, default True\n",
    "            Plots missing values as a heatmap\n",
    "        target: the target variable in the dataframe\n",
    "           The dependent variable in the dataset\n",
    "        percent: float, Default 70\n",
    "            Percentage of missing values to be in a column before it is eligible for removal.\n",
    "        method_encoding: the method for encoding variables before label-encoding them.\n",
    "            Value can be binary, onehot, sumencode\n",
    "        method_scaling: the method for scaling the dataset. The target variable is not scaled.\n",
    "            Value can be scale, normalize and normalize_outlier\n",
    "        method_sampling: the method for treating imbalanced dataset.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "        a dataframe of preprocessed columns. It is best to use this function after the text variables have been preprocessed.\n",
    "    This function is stand alone used for automatic feature engineering.\n",
    "    \n",
    "    It hash encodes the id and set it to be the index of the dataframe.\n",
    "    '''\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame/ numpy2d array, got 'None'\")\n",
    "    try:\n",
    "        ###balancing imbalance dataset\n",
    "        a = (data[target].value_counts()/data[target].count())*100\n",
    "        percent  = np.abs(a[0]-a[1])\n",
    "        if len(data[target].unique()) == 2 and percent > 20:\n",
    "            print(\"imbalanced datasets \")\n",
    "            print(\"working on balancing the datasets\")\n",
    "            a = pd.DataFrame(a).reset_index()\n",
    "            majority_c = a[a[target] == a[target].max()][\"index\"][0]\n",
    "            minority_c  =  a[a[target] == a[target].min()][\"index\"][0]\n",
    "            return  handle_imbalanced_data(data, method_sampling,target, majority_c, minority_c,number_of_samples = 2000 )\n",
    "        else:\n",
    "          print(\"this datasets is balanced\")\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "    #hash encoding ids\n",
    "    cols = get_cat_feats(data)\n",
    "    new_col = []\n",
    "    for x in cols:\n",
    "        if re.search(\"_id\", x.lower()) and data[x].nunique()>3:\n",
    "            string = str(data[x])\n",
    "            if bool(re.findall(r'\\d+', string)) == True:\n",
    "                new_col.append(x)\n",
    "    if len(new_col)>0:\n",
    "        data.set_index(pd.util.hash_pandas_object(data[new_col]), drop=False, inplace=True)\n",
    "    data = data.drop(new_col, axis=1)\n",
    "    data = drop_redundant(data)\n",
    "    data = deal_with_missing_value(data, percent=70)\n",
    "    X = data.drop(target, axis =1)\n",
    "    y = data[target]\n",
    "    if date_cols is not None:\n",
    "        data_dates = data[date_cols]\n",
    "        data_dates = date_processing(data=data_dates, date_cols = date_cols)\n",
    "        data_dates = data_dates.drop(date_cols, axis=1)\n",
    "        X = data.drop(date_cols, axis=1)\n",
    "    \n",
    "    print('Bivariant Stats between categorical features')\n",
    "    print(bivariate_stats(X))\n",
    "    _space()\n",
    "    try:\n",
    "        if target is not None:\n",
    "            print('Bivariant Stats between cat feats and target variable')\n",
    "            print(bivariate_stats_target(X, target))\n",
    "            _space()\n",
    "    except:\n",
    "        pass\n",
    "  \n",
    "    X = encode_data(X, method=method_encoding)\n",
    "   \n",
    "   \n",
    "    if data[target].dtype == 'object':\n",
    "        data[target] = le.fit_transform(data[target].astype(str))\n",
    "   \n",
    "    try:\n",
    "        if date_cols is not None:\n",
    "            df = pd.concat([X, data_dates, data[target]], axis=1)\n",
    "        else:\n",
    "            df = pd.concat([X, data[target]], axis=1)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        print('Pearson Correlation')\n",
    "        print(df.corr())\n",
    "        _space()\n",
    "        print('Kendall Correlation')\n",
    "        print(df.corr(method='kendall'))\n",
    "        _space()\n",
    "        print('Spearman Correlation')\n",
    "        print(df.corr(method='spearman'))\n",
    "        df = df[get_num_feats(data=df)]\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        corr = df.corr()\n",
    "        #fig, axe = plt.subplots(figsize=(12,9))\n",
    "        # fig = plt.figure(figsize=(200,14))\n",
    "        sns.set(rc={'figure.figsize':(18,18)}, font_scale=1.15)\n",
    "        mask = np.zeros_like(corr, dtype=np.bool)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "        with sns.axes_style('white'):\n",
    "            ax = sns.heatmap(\n",
    "                corr, \n",
    "                vmin=-1, vmax=1, center=0,\n",
    "                cmap='coolwarm',\n",
    "                square=True,\n",
    "                mask=mask,\n",
    "                annot=True,\n",
    "                fmt = '.2f'\n",
    "            )\n",
    "            ax.set_xticklabels(\n",
    "                ax.get_xticklabels(),\n",
    "                rotation=45,\n",
    "                horizontalalignment='right'\n",
    "            );\n",
    "    except:\n",
    "        pass\n",
    "    df2 = scale_normalize_data(df.drop(target, axis=1), cols = None, method = method_scaling)\n",
    "    df2 = pd.concat([df2, df[target].reset_index()], axis=1)\n",
    "    #df['hashed'] = df.index\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-p8aNzOpLW0w"
   },
   "source": [
    "\n",
    "## Handling imbalance datasets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XCUpJPqaMkZP"
   },
   "outputs": [],
   "source": [
    "#  Down-sample Majority Class\n",
    "def down_sample_majority_class(df,majority_class,minority_class,number_of_samples, target):\n",
    "\n",
    "    '''\n",
    "\n",
    "    df: pandas dataframe or series\n",
    "    majority_class: class with the majority class\n",
    "    minority_class:class with minority class\n",
    "    number_of_samples: number of samples to down sample to minority class\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Separate majority and minority classes\n",
    "    df_majority = df[df[target]==majority_class]\n",
    "    df_minority = df[df[target]==minority_class]\n",
    "    \n",
    "    # Downsample majority class\n",
    "    df_majority_downsampled = resample(df_majority, \n",
    "                                    replace=False,    # sample without replacement\n",
    "                                    n_samples=number_of_samples,     # to match minority class\n",
    "                                    random_state=123) # reproducible results\n",
    "    \n",
    "    # Combine minority class with downsampled majority class\n",
    "    df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "    \n",
    "    # Display new class counts\n",
    "    df_downsampled[target].value_counts()\n",
    "\n",
    "    return df_downsampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "shTq-ZUaMk59"
   },
   "outputs": [],
   "source": [
    "# random_under_sampling\n",
    "def random_under_sampling(df,majority_class,minority_class,target):\n",
    "    '''\n",
    "\n",
    "    df: pandas dataframe or series\n",
    "    majority_class: class with the majority class\n",
    "    minority_class:class with minority class\n",
    "    target: targeted class \n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    count_class_0, count_class_1 = df[target].value_counts()\n",
    "\n",
    "    # Separate majority and minority classes\n",
    "    df_majority = df[df[target]==majority_class]\n",
    "    df_minority = df[df[target]==minority_class]\n",
    "\n",
    "    # Random under-sampling\n",
    "    df_class_0_under = df_majority.sample(count_class_1)\n",
    "    df_test_under = pd.concat([df_class_0_under, df_minority], axis=0)\n",
    "\n",
    "\n",
    "    print('Random under-sampling:')\n",
    "    print(df_test_under[target].value_counts())\n",
    "\n",
    "    balanced_data  = df_test_under.copy()\n",
    "\n",
    "    return balaced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a7Oh1as0DZ4b"
   },
   "outputs": [],
   "source": [
    "# Up-sample Minority Class\n",
    "\n",
    "from sklearn.utils import resample\n",
    "def up_sample_minority_class(df,majority_class,minority_class,number_of_samples,target):\n",
    "\n",
    "  '''\n",
    "  df: pandas dataframe or series\n",
    "  majority_class: class with the majority class\n",
    "  minority_class:class with minority class\n",
    "  number_of_samples: number of samples to up sample the minority class too\n",
    "\n",
    "\n",
    "  '''\n",
    "\n",
    "  # Separate majority and minority classes\n",
    "  df_majority = df[df[target]==majority_class]\n",
    "  df_minority = df[df[target]==minority_class]\n",
    "  \n",
    "  # Upsample minority class\n",
    "  df_minority_upsampled = resample(df_minority, \n",
    "                                  replace=True,     # sample with replacement\n",
    "                                  n_samples=number_of_samples,    # to match majority class\n",
    "                                  random_state=123) # reproducible results\n",
    "  \n",
    "  # Combine majority class with upsampled minority class\n",
    "  df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "  \n",
    "  # Display new class counts\n",
    "  df_upsampled.target.value_counts()\n",
    "\n",
    "  balanced_data  = df_upsampled.copy()\n",
    "\n",
    "  return balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TsEIB7JNDZz7"
   },
   "outputs": [],
   "source": [
    "# #  random over sampling\n",
    "def random_over_sampling(df,majority_class,minority_class,target):\n",
    "\n",
    "    '''\n",
    "    performs a random over sampling on the datasets\n",
    "\n",
    "    df: pandas dataframe or series\n",
    "    majority_class: class with the majority class\n",
    "    minority_class:class with minority class\n",
    "    target: targeted class \n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    count_class_0, count_class_1 = df[target].value_counts()\n",
    "\n",
    "    # Separate majority and minority classes\n",
    "    df_majority = df[df[target]==majority_class]\n",
    "    df_minority = df[df[target]==minority_class]\n",
    "\n",
    "    df_class_1_over = df_minority.sample(count_class_0, replace=True)\n",
    "    df_test_over = pd.concat([df_class_1_over, df_majority], axis=0)\n",
    "\n",
    "    print('Random over-sampling:')\n",
    "    print(df_test_over.target.value_counts())\n",
    "\n",
    "    df_test_over[target].value_counts().plot(kind='bar', title='Count (target)');\n",
    "\n",
    "    balanced_data  = df_test_over\n",
    "\n",
    "    return balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7HfWBSRxLxla"
   },
   "outputs": [],
   "source": [
    "def handle_imbalanced_data(df, method='up_sample_minority_class',target = None,majority_class= 0,minority_class= 1,number_of_samples= 3000):\n",
    "    \"\"\"\n",
    "        handles imbalanced data\n",
    "        method takes either up_sample_minority_class or random_over_sampling or down_sample_majority_class or random_under_sampling. default is up_sample_minority_class\n",
    "        \n",
    "        this function can be used alone and it also used in the feature_processing function.\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    if method == 'up_sample_minority_class':\n",
    "        data = up_sample_minority_class(df,majority_class,minority_class,number_of_samples,target)\n",
    "    elif method== 'random_over_sampling':\n",
    "        data  = random_over_sampling(df,majority_class,minority_class,target)\n",
    "    elif method == 'down_sample_majority_class':\n",
    "        data = down_sample_majority_class(df,majority_class,minority_class,number_of_samples,target)\n",
    "    elif method  == \"random_under_sampling\":\n",
    "        data = random_under_sampling(df,majority_class,minority_class,target)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d7DVsvAbLxaJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-RwtcD7dWD7H",
    "outputId": "84f10c79-4044-45bc-9977-943ecdb419fc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ThkcU4ApWDsb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "avNG1-Yw-UHv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YmO2P5Hh-UH-",
    "outputId": "0f32f818-6595-4bef-c07c-7a34a1ddb119"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AmMDEkVuyZUx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HnMpemF-zaj9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CesL1O-dzac1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ai8E7cHbzaSx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DXpaxEd61Oe_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eQyx9-yr8PGu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VZ4LSiJl8O6p"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xJMV770985Dp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lBrFZBFC1OUg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ra9BX53p-UIf"
   },
   "source": [
    "### Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BqBVNkUf-UIp",
    "outputId": "67958274-114a-48eb-ad0d-4849571d400d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zh1ifsUs-UJE",
    "outputId": "cf747fac-abf5-436e-ad40-8054e3b25774"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ighdaro Emwinghare\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\Ighdaro Emwinghare\\Downloads\\data2016_16f.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EsiqzfuT-UJd",
    "outputId": "4e123ed6-e38e-4e14-a779-3f7a65cd853f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['npi',\n",
       " 'nppes_credentials',\n",
       " 'nppes_provider_gender',\n",
       " 'nppes_entity_code',\n",
       " 'nppes_provider_zip',\n",
       " 'provider_type',\n",
       " 'medicare_participation_indicator',\n",
       " 'place_of_service',\n",
       " 'hcpcs_code',\n",
       " 'hcpcs_drug_indicator',\n",
       " 'line_srvc_cnt',\n",
       " 'bene_unique_cnt',\n",
       " 'bene_day_srvc_cnt',\n",
       " 'average_medicare_allowed_amt',\n",
       " 'average_submitted_chrg_amt',\n",
       " 'average_medicare_payment_amt']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xv8MJKfM-UJ0",
    "outputId": "edf02aef-4da2-4a96-8a89-42cf7e110e94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heat  map\n",
      "count checker\n",
      "==npi==\n",
      "data:0.10302982142063075\n",
      "==nppes_credentials==\n",
      "data:0.001589106049102327\n",
      "==nppes_provider_gender==\n",
      "data:2.058694194976457e-07\n",
      "==nppes_entity_code==\n",
      "data:2.058694194976457e-07\n",
      "==nppes_provider_zip==\n",
      "data:0.03264965471580962\n",
      "==provider_type==\n",
      "data:9.367058587142879e-06\n",
      "==medicare_participation_indicator==\n",
      "data:2.058694194976457e-07\n",
      "==place_of_service==\n",
      "data:2.058694194976457e-07\n",
      "==hcpcs_code==\n",
      "data:0.0006198728221074112\n",
      "==hcpcs_drug_indicator==\n",
      "data:2.058694194976457e-07\n",
      "==line_srvc_cnt==\n",
      "data:0.0031889173080185316\n",
      "==bene_unique_cnt==\n",
      "data:0.0009736594195141153\n",
      "==bene_day_srvc_cnt==\n",
      "data:0.0012535388953211644\n",
      "==average_medicare_allowed_amt==\n",
      "data:0.2922832112664922\n",
      "==average_submitted_chrg_amt==\n",
      "data:0.21005731816377654\n",
      "==average_medicare_payment_amt==\n",
      "data:0.5840839675483916\n",
      "cat  plots\n"
     ]
    }
   ],
   "source": [
    "describe(data=data, name='', date_cols=['period_week','period_month', 'date'], show_categories=False, plot_missing=False, target='product')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qEek2AXN-UKd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "woQ9u0qW-UKp",
    "outputId": "6a4e69f8-e8ba-4ede-fb61-8c93431b0f31"
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NkRBFbjH-ULH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sce-wusQ-UL0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AkH_sfxJ-UM6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8aUUP7Xd-UN3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWfK4kya-UOq",
    "outputId": "41975aeb-85de-4b38-ef61-4b9f75989cd7"
   },
   "outputs": [],
   "source": [
    "data1 = feature_processing(data=data, target='average_medicare_payment_amt', percent = 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F6C5na7W-UO-",
    "outputId": "496e7a52-05b4-421b-eba8-75faf5024e4c"
   },
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZTFq2_zS-UPI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0jiwDeba-UPR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A-CgLzDx-UPc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7gEdGN3f-UPl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QEGImrCQ-UP8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "05CRTjic-UQR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4GRx-gxj-UQl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2WXkR9uI-UQy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9PJ_AN4B-URB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fp5I27QT-URM"
   },
   "source": [
    "### Notes.\n",
    "\n",
    "1. Added New functions for scaling and normalising dataset. Then a master function 'scale_normalize_data' that use standard_scaler for normalization as default. It can also use robust_scaler when method is set to 'normalize_outlier'. It also uses minmax_scalere for scaling when method is set to'scale'. \n",
    "2. Made change to the feature_processing function to only output numeric columns as part of the dataframe. \n",
    "\n",
    "3. The 3 master functions are **describe** and **feature_processing**  functions.\n",
    "4. The hashing of the ids is done by the **feature_processing** function.\n",
    "5. Added a part for ml modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6c7UT94SLbfG"
   },
   "source": [
    "## MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "R-BX4qQ6Y_b4",
    "outputId": "b713fa8e-2530-4aeb-f86c-7feef6c8b6e8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "avRZPxBr5ACr",
    "outputId": "0f052cb3-65c4-4d44-e306-a8142b299e19"
   },
   "outputs": [],
   "source": [
    "def RandomForestRegressor(X,y,kfold = False,n_estimators = 1000,subsample =0.8,num_parallel_tree=100,max_depth=5,splits=2,colsample_bynode= 0.8,learning_rate = 1):\n",
    "    ''' \n",
    "    train the datsets with random forest regressor\n",
    "    X: datasets to train the model\n",
    "    y : target feature\n",
    "    n_estimators specifies the size of the forest to be trained; \n",
    "    it is converted to num_parallel_tree, instead of the number of boosting rounds\n",
    "    learning_rate is set to 1 by default\n",
    "    colsample_bynode and subsample are set to 0.8 by default\n",
    "    booster is always gbtree\n",
    "    splits: number of kfold splits\n",
    "    '''\n",
    "    \n",
    "\n",
    "    params = {\n",
    "        'n_estimators':n_estimators,\n",
    "    'colsample_bynode': colsample_bynode,\n",
    "    'learning_rate': learning_rate,\n",
    "    'max_depth': max_depth,\n",
    "    'num_parallel_tree': num_parallel_tree,\n",
    "    'subsample': subsample,\n",
    "    'tree_method': 'gpu_hist'\n",
    "    }\n",
    "\n",
    "    if kfold == True:\n",
    "        kf = KFold(n_splits=splits)\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "            xgb_model = xgb.XGBRFRegressor(params= params , random_state=42).fit(X.iloc[list(train_index)], y.iloc[list(train_index)])\n",
    "\n",
    "        pred = xgb_model.predict(X.iloc[list(test_index)])\n",
    "        print(\"predicted values are\")\n",
    "        print(pred)\n",
    "        print(\"mean absolute error\")\n",
    "        print(mean_absolute_error(y.iloc[list(test_index)],pred))\n",
    "    elif kfold ==False:\n",
    "     \n",
    "        xgb_model = xgb.XGBRFRegressor(params= params , random_state=42).fit(X_train,y_train)\n",
    "        pred = xgb_model.predict(X_test)\n",
    "        print(\"predicted values are\")\n",
    "        print(pred)\n",
    "        print(\"mean absolute error\")\n",
    "        print(mean_absolute_error(y_test,pred))\n",
    "    else:\n",
    "        print(\"wrong choice please let your options be between true and false\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c7fy_YjT6kx8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bx7aErjEY_6_"
   },
   "outputs": [],
   "source": [
    "def RandomForestclassifier(X,y,kfold = False,n_estimators = 1000,subsample =0.8,num_parallel_tree=100,max_depth=5,splits=2,colsample_bynode= 0.8,learning_rate = 1):\n",
    "    ''' \n",
    "    train the datsets with random forest classifier\n",
    "    X: datasets to train the model\n",
    "    y : target feature\n",
    "    n_estimators specifies the size of the forest to be trained; \n",
    "    it is converted to num_parallel_tree, instead of the number of boosting rounds\n",
    "    learning_rate is set to 1 by default\n",
    "    colsample_bynode and subsample are set to 0.8 by default\n",
    "    booster is always gbtree\n",
    "    splits: number of kfold splits\n",
    "    '''\n",
    "    \n",
    "\n",
    "    params = {\n",
    "        'n_estimators':n_estimators,\n",
    "    'colsample_bynode': colsample_bynode,\n",
    "    'learning_rate': learning_rate,\n",
    "    'max_depth': max_depth,\n",
    "      'objective': 'binary:logistic',\n",
    "\n",
    "    'num_parallel_tree': num_parallel_tree,\n",
    "    'subsample': subsample,\n",
    "    'tree_method': 'gpu_hist'\n",
    "    }\n",
    "\n",
    "    if kfold == True:\n",
    "        kf = KFold(n_splits=splits)\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "            xgb_model = xgb.XGBRFClassifier(params= params , random_state=42).fit(X.iloc[list(train_index)], y.iloc[list(train_index)])\n",
    "\n",
    "        pred = xgb_model.predict(X.iloc[list(test_index)])\n",
    "        print(\"predicted values are\")\n",
    "        print(pred)\n",
    "        print(\"accuracy score\")\n",
    "        print(mean_absolute_error(y.iloc[list(test_index)],pred))\n",
    "    elif kfold ==False:\n",
    "        xgb_model = xgb.XGBRFClassifier(params= params , random_state=42).fit(X_train,y_train)\n",
    "        pred = xgb_model.predict(X_test)\n",
    "        print(\"predicted values are\")\n",
    "        print(pred)\n",
    "        print(\"accuracy score\")\n",
    "        print(accuracy_score(y_test,pred))\n",
    "    else:\n",
    "        print(\"wrong choice please let your options be between true and false\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "ngyPJZ4l-URf",
    "outputId": "73c61cd2-cad8-4385-8483-a0182b6cb6e7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gNLX0lGx-aqt"
   },
   "outputs": [],
   "source": [
    "def Xgboostclassifier(X,y,kfold = False,n_estimators = 1000,subsample =0.8,num_parallel_tree=100,max_depth=5,splits=2,colsample_bynode= 0.8,learning_rate = 1):\n",
    "   ''' \n",
    "    train the datsets with xgboost classifier\n",
    "    X: datasets to train the model\n",
    "    y : target feature\n",
    "    n_estimators specifies the size of the forest to be trained; \n",
    "    it is converted to num_parallel_tree, instead of the number of boosting rounds\n",
    "    learning_rate is set to 1 by default\n",
    "    colsample_bynode and subsample are set to 0.8 by default\n",
    "    booster is always gbtree\n",
    "    splits: number of kfold splits\n",
    "    '''\n",
    "    \n",
    "\n",
    "    params = {\n",
    "        'n_estimators':n_estimators,\n",
    "    'colsample_bynode': colsample_bynode,\n",
    "    'learning_rate': learning_rate,\n",
    "    'max_depth': max_depth,\n",
    "      'objective': 'binary:logistic',\n",
    "\n",
    "    'num_parallel_tree': num_parallel_tree,\n",
    "    'subsample': subsample,\n",
    "    'tree_method': 'gpu_hist'\n",
    "    }\n",
    "\n",
    "    if kfold == True:\n",
    "        kf = KFold(n_splits=splits)\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "            xgb_model = xgb.XGBRFClassifier(params= params , random_state=42).fit(X.iloc[list(train_index)], y.iloc[list(train_index)])\n",
    "\n",
    "        pred = xgb_model.predict(X.iloc[list(test_index)])\n",
    "        print(\"predicted values are\")\n",
    "        print(pred)\n",
    "        print(\"accuracy score\")\n",
    "        print(mean_absolute_error(y.iloc[list(test_index)],pred))\n",
    "    elif kfold ==False:\n",
    "        xgb_model = xgb.XGBClassifier(params= params , random_state=42).fit(X_train,y_train)\n",
    "        pred = xgb_model.predict(X_test)\n",
    "        print(\"predicted values are\")\n",
    "        print(pred)\n",
    "        print(\"accuracy score\")\n",
    "        print(accuracy_score(y_test,pred))\n",
    "    else:\n",
    "        print(\"wrong choice please let your options be between true and false\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "GPB9fLxg-ad4",
    "outputId": "2ccedf35-940c-4a01-e991-76792407b852"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VSR9s0OQ-53K"
   },
   "outputs": [],
   "source": [
    "def xgboostRegressor(X,y,kfold = False,n_estimators = 1000,subsample =0.8,num_parallel_tree=100,max_depth=5,splits=2,colsample_bynode= 0.8,learning_rate = 1):\n",
    "    ''' \n",
    "    train the dataset with xgboost regressor\n",
    "    X: datasets to train the model\n",
    "    y : target feature\n",
    "    n_estimators specifies the size of the forest to be trained; \n",
    "    it is converted to num_parallel_tree, instead of the number of boosting rounds\n",
    "    learning_rate is set to 1 by default\n",
    "    colsample_bynode and subsample are set to 0.8 by default\n",
    "    booster is always gbtree\n",
    "    splits: number of kfold splits\n",
    "    '''\n",
    "    \n",
    "\n",
    "    params = {\n",
    "        'n_estimators':n_estimators,\n",
    "    'colsample_bynode': colsample_bynode,\n",
    "    'learning_rate': learning_rate,\n",
    "    'max_depth': max_depth,\n",
    "    'num_parallel_tree': num_parallel_tree,\n",
    "    'subsample': subsample,\n",
    "    'tree_method': 'gpu_hist'\n",
    "    }\n",
    "\n",
    "    if kfold == True:\n",
    "        kf = KFold(n_splits=splits)\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "            xgb_model = xgb.XGBRegressor(params= params , random_state=42).fit(X.iloc[list(train_index)], y.iloc[list(train_index)])\n",
    "\n",
    "        pred = xgb_model.predict(X.iloc[list(test_index)])\n",
    "        print(\"predicted values are\")\n",
    "        print(pred)\n",
    "        print(\"mean absolute error\")\n",
    "        print(mean_absolute_error(y.iloc[list(test_index)],pred))\n",
    "    elif kfold ==False:\n",
    "     \n",
    "        xgb_model = xgb.XGBRegressor(params= params , random_state=42).fit(X_train,y_train)\n",
    "        pred = xgb_model.predict(X_test)\n",
    "        print(\"predicted values are\")\n",
    "        print(pred)xgboostRegressor\n",
    "        print(\"mean absolute error\")\n",
    "        print(mean_absolute_error(y_test,pred))\n",
    "    else:\n",
    "        print(\"wrong choice please let your options be between true and false\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uk_8XGpmlVOX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yRtvwWn3kyTC"
   },
   "outputs": [],
   "source": [
    "def svmRegressor(X,y,kfold = False,kernel='linear', degree=3):\n",
    "   ''' \n",
    "    trains the model with support vector regressor\n",
    "    X: datasets to train the model\n",
    "    y: target features\n",
    "    kfold: number of folds\n",
    "    kernels: speify the type of kernel\n",
    "    degree: number of degree\n",
    "    '''\n",
    "    \n",
    "\n",
    "  \n",
    "\n",
    "    if kfold == True:\n",
    "        kf = KFold(n_splits=splits)\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "            model = svm.SVR( kernel=kernel,degree=degree).fit(X.iloc[list(train_index)], y.iloc[list(train_index)])\n",
    "\n",
    "        pred = model.predict(X.iloc[list(test_index)])\n",
    "        print(\"predicted values are\")\n",
    "        print(pred)\n",
    "        print(\"mean absolute error\")\n",
    "        print(mean_absolute_error(y.iloc[list(test_index)],pred))\n",
    "    elif kfold ==False:\n",
    "     \n",
    "        model = svm.SVR(kernel=kernel,degree=degree ).fit(X_train,y_train)\n",
    "        pred = model.predict(X_test)\n",
    "        print(\"predicted values are\")\n",
    "        print(pred)\n",
    "        print(\"mean absolute error\")\n",
    "        print(mean_absolute_error(y_test,pred))\n",
    "    else:\n",
    "        print(\"wrong choice please let your options be between true and false\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "Aw0JKUbRlFhX",
    "outputId": "0ce22a74-f408-4f55-da99-99946ac16837"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EJwGxQq3mVSh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "fp5I27QT-URM"
   ],
   "name": "Statiscal_Analysis_04_08_2020_2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
