{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Text Analytics_July_29th.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHeEaMXFZpww",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "754f7d6c-95aa-49b6-a0e7-42b78a41bb60"
      },
      "source": [
        "!pip install langid"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting langid\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/4c/0fb7d900d3b0b9c8703be316fbddffecdab23c64e1b46c7a83561d78bd43/langid-1.1.6.tar.gz (1.9MB)\n",
            "\r\u001b[K     |▏                               | 10kB 15.0MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 6.7MB/s eta 0:00:01\r\u001b[K     |▌                               | 30kB 8.4MB/s eta 0:00:01\r\u001b[K     |▊                               | 40kB 8.4MB/s eta 0:00:01\r\u001b[K     |▉                               | 51kB 7.0MB/s eta 0:00:01\r\u001b[K     |█                               | 61kB 7.2MB/s eta 0:00:01\r\u001b[K     |█▏                              | 71kB 8.1MB/s eta 0:00:01\r\u001b[K     |█▍                              | 81kB 8.5MB/s eta 0:00:01\r\u001b[K     |█▌                              | 92kB 8.3MB/s eta 0:00:01\r\u001b[K     |█▊                              | 102kB 8.5MB/s eta 0:00:01\r\u001b[K     |█▉                              | 112kB 8.5MB/s eta 0:00:01\r\u001b[K     |██                              | 122kB 8.5MB/s eta 0:00:01\r\u001b[K     |██▏                             | 133kB 8.5MB/s eta 0:00:01\r\u001b[K     |██▍                             | 143kB 8.5MB/s eta 0:00:01\r\u001b[K     |██▌                             | 153kB 8.5MB/s eta 0:00:01\r\u001b[K     |██▊                             | 163kB 8.5MB/s eta 0:00:01\r\u001b[K     |███                             | 174kB 8.5MB/s eta 0:00:01\r\u001b[K     |███                             | 184kB 8.5MB/s eta 0:00:01\r\u001b[K     |███▎                            | 194kB 8.5MB/s eta 0:00:01\r\u001b[K     |███▍                            | 204kB 8.5MB/s eta 0:00:01\r\u001b[K     |███▋                            | 215kB 8.5MB/s eta 0:00:01\r\u001b[K     |███▊                            | 225kB 8.5MB/s eta 0:00:01\r\u001b[K     |████                            | 235kB 8.5MB/s eta 0:00:01\r\u001b[K     |████                            | 245kB 8.5MB/s eta 0:00:01\r\u001b[K     |████▎                           | 256kB 8.5MB/s eta 0:00:01\r\u001b[K     |████▍                           | 266kB 8.5MB/s eta 0:00:01\r\u001b[K     |████▋                           | 276kB 8.5MB/s eta 0:00:01\r\u001b[K     |████▊                           | 286kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 296kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 307kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 317kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 327kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 337kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 348kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 358kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 368kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 378kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 389kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 399kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 409kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 419kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 430kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 440kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 450kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 460kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 471kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 481kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 491kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 501kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 512kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 522kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 532kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 542kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 552kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 563kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 573kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 583kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 593kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 604kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 614kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 624kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 634kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 645kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 655kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 665kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 675kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 686kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 696kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 706kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 716kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 727kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 737kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 747kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 757kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 768kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 778kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 788kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 798kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 808kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 819kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 829kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 839kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 849kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 860kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 870kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 880kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 890kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 901kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 911kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 921kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 931kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 942kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 952kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 962kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 972kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 983kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 993kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.0MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.0MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.0MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.0MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.0MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.1MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.1MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.1MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.1MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.1MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.1MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.1MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.1MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.1MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.1MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.2MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.2MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.2MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.2MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.2MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.2MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.2MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.2MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.2MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.2MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.3MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.3MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.3MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.3MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.3MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.3MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.3MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.3MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.3MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.4MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.4MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.4MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.4MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.4MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.4MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.4MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.4MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.4MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.4MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.5MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.5MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.5MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.5MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.5MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.5MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.5MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.5MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.5MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.5MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.6MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.6MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.6MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.6MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.6MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.6MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.6MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.6MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.6MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.6MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.7MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.7MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.7MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.7MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.7MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.7MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.7MB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.7MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.7MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.8MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.8MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.8MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.8MB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.8MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.8MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.8MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.8MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.8MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.8MB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.9MB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.9MB 25kB/s eta 0:00:03\r\u001b[K     |███████████████████████████████▏| 1.9MB 25kB/s eta 0:00:03\r\u001b[K     |███████████████████████████████▎| 1.9MB 25kB/s eta 0:00:02\r\u001b[K     |███████████████████████████████▌| 1.9MB 25kB/s eta 0:00:02\r\u001b[K     |███████████████████████████████▋| 1.9MB 25kB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.9MB 25kB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.9MB 25kB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.9MB 25kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from langid) (1.18.5)\n",
            "Building wheels for collected packages: langid\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-cp36-none-any.whl size=1941190 sha256=d2ba024fcfec22b91bd7ba8ac8281712164753c166230dccc3d52764dfd0c6cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/bc/61/50a93be85d1afe9436c3dc61f38da8ad7b637a38af4824e86e\n",
            "Successfully built langid\n",
            "Installing collected packages: langid\n",
            "Successfully installed langid-1.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnyMupJjakUw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "de8a9378-b5d8-4702-fad3-79b7c67d31d6"
      },
      "source": [
        "!pip install normalise"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting normalise\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/2d/f06cf3d3714502dec10e19238a5da201b71ce198165beda9c1adaf5063da/normalise-0.1.8-py3-none-any.whl (15.7MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7MB 295kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from normalise) (1.18.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from normalise) (3.2.5)\n",
            "Collecting roman\n",
            "  Downloading https://files.pythonhosted.org/packages/c3/9e/47df0bf47ccd7e9bbbf0a539ac86e45ded37c34dba544a0a2e5d01ce5f88/roman-3.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from normalise) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from normalise) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->normalise) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->normalise) (0.16.0)\n",
            "Installing collected packages: roman, normalise\n",
            "Successfully installed normalise-0.1.8 roman-3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrACEmPSbRVm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5b41d74f-5928-40f1-f146-ebf18500cf8a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('brown')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWf70hpDbbTE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f1bde5be-d8ed-449c-ba9d-f14b37a8e8e7"
      },
      "source": [
        "import nltk\n",
        "nltk.download('names')  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nMZ7eYzZpxO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "032cf886-3074-47c3-b7db-daa3046d8e68"
      },
      "source": [
        "import langid\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "from normalise import normalise\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "# importing the model en_core_web_sm of English for vocabluary, syntax & entities\n",
        "import en_core_web_sm\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "import gensim\n",
        "from gensim.summarization.summarizer import summarize \n",
        "from gensim.summarization import keywords \n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "import spacy\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# load en_core_web_sm of English for vocabluary, syntax & entities\n",
        "nlp = en_core_web_sm.load()\n",
        "import pandas as pd\n",
        "from nltk.stem import SnowballStemmer\n",
        "from collections import defaultdict\n",
        "# Import Heapq \n",
        "from heapq import nlargest\n",
        "# Load Pkgs\n",
        "import spacy\n",
        "# Text Preprocessing Pkg\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation\n",
        "# Build a List of Stopwords\n",
        "stopwords = list(STOP_WORDS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.semi_supervised.label_propagation module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.semi_supervised. Anything that cannot be imported from sklearn.semi_supervised is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelPropagation from version 0.18 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-98994683dae3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCoherenceModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t6aMKebZpxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V3DgHpJZpyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "langid.classify(\"Эй, чувак, как ты\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU0BgrQxZpyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classify_lang(text):\n",
        "    '''This function will be applied to a series using .apply.\n",
        "        It classifies the language.\n",
        "    '''\n",
        "    lang = langid.classify(text)[0]\n",
        "    return lang"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkcxIoFGZpyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_lower_case(data):\n",
        "    return np.char.lower(data)\n",
        "\n",
        "def remove_stopwords(data):\n",
        "    stop_words = stopwords.words('english')\n",
        "    words = word_tokenize(str(data))\n",
        "    new_text = \"\"\n",
        "    for w in words:\n",
        "        if w not in stop_words and len(w) > 1:\n",
        "            new_text = new_text + \" \" + w\n",
        "    return new_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UNaTncEZpzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_punctuation(mess):\n",
        "    nopunc =[char for char in mess if char not in string.punctuation]\n",
        "    nopunc=''.join(nopunc)\n",
        "\n",
        "    return nopunc\n",
        "\n",
        "\n",
        "def remove_apostrophe(data):\n",
        "    return np.char.replace(data, \"'\", \"\")\n",
        "\n",
        "\n",
        "def stemming(data):\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    tokens = word_tokenize(str(data))\n",
        "    new_text = \"\"\n",
        "    for w in tokens:\n",
        "        new_text = new_text + \" \" + stemmer.stem(w)\n",
        "    return new_text\n",
        "def lemmatizer(text):        \n",
        "    sent = []\n",
        "    doc = nlp(text)\n",
        "    for word in doc:\n",
        "        sent.append(word.lemma_)\n",
        "    return \" \".join(sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO0a2ZA2Zpze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flatten_list(list_of_list):\n",
        "    flat_list = [item for sublist in list_of_list for item in sublist]\n",
        "    return flat_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlMW8R86Zpz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_similarity(sentence1, sentence2, stopwords = nltk.corpus.stopwords.words('english')):\n",
        "    \n",
        "    \"\"\" \n",
        "    Finds the Lexical similarity between two sentences based on cosine similarity\n",
        "    \n",
        "    Parameters :-\n",
        "    ------------------------------------\n",
        "          sentence1 (str) : First sentence\n",
        "          sentence2 (str) : Second sentence that is compared to the first sentence\n",
        "          stopwords (list) : List of stopwords for filtering commonly used words\n",
        "          \n",
        "    Returns :-\n",
        "    -------------------------------------\n",
        "          similarity (float) : Similarity index between the two sentences represented in a probabilistic form (0.0 <= x <= 1.0)      \n",
        "    \"\"\"\n",
        "    if stopwords == None:\n",
        "        stopwords = []\n",
        "        \n",
        "    assert len(sentence1) > 0 and len(sentence2) > 0, \"Each sentence must contain at least one word !\"\n",
        "    \n",
        "\n",
        "    sent1 = [word.lower() for word in sentence1.split(\" \")]\n",
        "    sent2 = [word.lower() for word in sentence2.split(\" \")]\n",
        "\n",
        "    all_words = list(set(sent1+sent2))\n",
        "    vect_array1 = np.zeros(len(all_words))\n",
        "    vect_array2 = np.zeros(len(all_words))\n",
        "\n",
        "    for word in sent1:\n",
        "        if word in stopwords:\n",
        "            continue\n",
        "        else:\n",
        "            vect_array1[all_words.index(word)] += 1\n",
        "\n",
        "    for word in sent2:\n",
        "        if word in stopwords:\n",
        "            continue\n",
        "        else:\n",
        "            vect_array2[all_words.index(word)] += 1\n",
        "    \n",
        "    similarity = np.round(1 - cosine_distance(vect_array1, vect_array2), 3)\n",
        "\n",
        "    return similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf5iBBjvZp0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_newline(text):\n",
        "    \"\"\"\n",
        "        Remove newline character and clean text particularly extracted from documents\n",
        "        Parameters :\n",
        "        --------------------------------------------------\n",
        "        text (str) : Text to be cleaned\n",
        "        Returns :\n",
        "        --------------------------------------------------\n",
        "        sentence\n",
        "    \"\"\"\n",
        "    cleaned_text = []\n",
        "    for i in text.split():\n",
        "        if i == \"\\n\":\n",
        "            continue\n",
        "        else: \n",
        "            cleaned_text.append(re.sub(r'[^a-zA-Z0-9_]', \"\", i))\n",
        "    \n",
        "    return \" \".join([x for x in cleaned_text if x.strip()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwU_fJVUZp0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_unwanted_chars(sentence):\n",
        "\n",
        "    \"\"\"\n",
        "        Remove unwanted Characters in-order to clean raw text data in tweets and html docs particularly\n",
        "        Paramters :\n",
        "        -----------------------------------------------------\n",
        "        sentence (str)\n",
        "    \"\"\"\n",
        "    mentions_pattern = r'@[A-Za-z0-9_]+'\n",
        "    http_pattern = r'http(s?)://[^ ]+'\n",
        "    www_pattern = r'www.[^ ]+'\n",
        "    non_alphabets = r'[^a-zA-Z]+'\n",
        "    combined_pattern_removal = r'|'.join((mentions_pattern, http_pattern, www_pattern, non_alphabets))\n",
        "    return re.sub(combined_pattern_removal, \" \", sentence).strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kDSYluQZp0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "def clean_text(text):\n",
        "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
        "    if text is None:\n",
        "        return ''\n",
        "    text = str(text).replace(\"nan\",'').lower()\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "    text = text.strip()\n",
        "    # Remove a sentence if it is only one word long\n",
        "    if len(text) >= 2:\n",
        "        return ' '.join(word for word in text.split() if word not in STOPWORDS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSeulpA_Zp1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split sentences\n",
        "def sentences(text):\n",
        "    \"\"\"extract sentences from the text.\"\"\"\n",
        "    # split sentences and questions\n",
        "    text = re.split('[.?]', text)\n",
        "    clean_sent = []\n",
        "    for sent in text:\n",
        "        clean_sent.append(sent)\n",
        "    return clean_sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxfxHmoEZp1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def part_of_speech(text):\n",
        "    \"\"\"gives the part of speech the different segment in the text.\"\"\"\n",
        "    # POS tagging\n",
        "\n",
        "    #  \"nlp\" Objectis used to create documents with linguistic annotations.\n",
        "    docs = nlp(text)\n",
        "\n",
        "    for word in docs:\n",
        "        return word.text, word.pos_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ_CI5lWZp10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(sent):\n",
        "    '''Function word tokenize the data and then give the part of speech of the subject, verb and object.'''\n",
        "    sent = nltk.word_tokenize(sent)\n",
        "    sent = nltk.pos_tag(sent)\n",
        "    return sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy2mjAGPZp2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def entity_extraction(doc):\n",
        "    \"\"\"extract the entity in the sentence. Typically can be a person, an organisation, a date, an event etc.\"\"\"\n",
        "    doc = nlp(doc)\n",
        "    for X in doc.ents:\n",
        "        return X.text, X.label_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cREYOaIaZp2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data = pd.read_csv(r\"C:\\Users\\Ighdaro Emwinghare\\Downloads\\data1.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUr3DsE5Zp23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSdG7qoCZp3Q",
        "colab_type": "code",
        "colab": {},
        "outputId": "1c21aa16-bb71-415e-b75e-db6276bbd672"
      },
      "source": [
        "data['content_message_local'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RINVOQ+MTX erstmals signifikant hoehere Remissionsraten vs. Adalimumab + MTX    18491\n",
              "(No message)                                                                    11938\n",
              "Verschiebe die Grenzen                                                           9199\n",
              "Local Key Message Does not apply                                                 4252\n",
              "Demonstrated Safety Profile                                                      3927\n",
              "                                                                                ...  \n",
              "Enrollment                                                                          1\n",
              "Dosiahnutie remisie vs placebo+MTX a ADA+MTX                                        1\n",
              "Safety: Similar to ustekinumab; no TB reactivation; no lab monitoring; ISI          1\n",
              "Favourable safety profile: A favourable safety profile at Week 16 and 52            1\n",
              "SKYRIZI safety profile                                                              1\n",
              "Name: content_message_local, Length: 122, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meyChEoFZp3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['content_message_local'] = data['content_message_local'].apply(lambda x: str(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uIt5p5rZp3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = data['content_message_local'].dropna(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzLCpor-Zp4B",
        "colab_type": "code",
        "colab": {},
        "outputId": "b22cab66-b13e-437b-bca6-2b3ee14337e5"
      },
      "source": [
        "type(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqUmsknlZp4V",
        "colab_type": "code",
        "colab": {},
        "outputId": "eb488877-f3e2-4c18-d7e5-25ef000b14a1"
      },
      "source": [
        "x.dtype"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('O')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YA7bLpjZp4l",
        "colab_type": "code",
        "colab": {},
        "outputId": "7838704e-974d-48c7-d922-3a67e675fb07"
      },
      "source": [
        "x.apply(preprocess)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10                  [(доказанное, NN), (превосходство, NN)]\n",
              "12                  [(доказанное, NN), (превосходство, NN)]\n",
              "24        [(remission, NN), (rates, NNS), (vs, VBP), (pl...\n",
              "25                                          [(message, NN)]\n",
              "26        [(safety, NN), (profile, NN), (robust, JJ), (t...\n",
              "                                ...                        \n",
              "168524         [(verschiebe, NN), (die, NN), (grenzen, NN)]\n",
              "168525    [(besseres, NNS), (klinische, VBP), (anspreche...\n",
              "168526    [(besseres, NNS), (klinische, VBP), (anspreche...\n",
              "168527    [(besseres, NNS), (klinische, VBP), (anspreche...\n",
              "168528    [(rinvoqmtx, NN), (erstmals, NNS), (signifikan...\n",
              "Name: content_message_local, Length: 113304, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0_fSJx4Zp42",
        "colab_type": "code",
        "colab": {},
        "outputId": "40d96f14-c59f-4322-f744-26f3ef2072a2"
      },
      "source": [
        "x.apply(classify_lang)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10        ru\n",
              "12        ru\n",
              "24        es\n",
              "25        en\n",
              "26        en\n",
              "          ..\n",
              "168524    de\n",
              "168525    de\n",
              "168526    de\n",
              "168527    de\n",
              "168528    de\n",
              "Name: content_message_local, Length: 113304, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJxy-eLzZp5E",
        "colab_type": "code",
        "colab": {},
        "outputId": "50a9804d-e1b3-4cb3-e371-2d1ebb639d0e"
      },
      "source": [
        "x.apply(entity_extraction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10                                None\n",
              "12                                None\n",
              "24               (mtx ada mtx, PERSON)\n",
              "25                                None\n",
              "26                                None\n",
              "                      ...             \n",
              "168524                            None\n",
              "168525    (besseres klinische, PERSON)\n",
              "168526    (besseres klinische, PERSON)\n",
              "168527    (besseres klinische, PERSON)\n",
              "168528                            None\n",
              "Name: content_message_local, Length: 113304, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejr2gG95Zp5V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_normalize(text, variety=\"BrE\",user_abbrevs={}, verbose=False ):\n",
        "    \"\"\"\n",
        "        variety - format of date (AmE - american type, BrE - british format)\n",
        "        user_abbrevs - dicts of abbreviations mappings (from normalise package)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = normalise(text, variety=variety, user_abbrevs= user_abbrevs, verbose = verbose)\n",
        "        \n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return text\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_dgO6fcZp5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spacy.matcher import Matcher\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "def extract_phone_number(nlp_doc):\n",
        "  \"\"\" extracts phone number from the input text\"\"\"\n",
        "    nlp_doc = nlp(nlp_doc)\n",
        "    pattern = [{'ORTH': '('}, {'SHAPE': 'ddd'},\n",
        "                {'ORTH': ')'}, {'SHAPE': 'ddd'},\n",
        "                {'ORTH': '-', 'OP': '?'},\n",
        "                {'SHAPE': 'ddd'}]\n",
        "    matcher.add('PHONE_NUMBER', None, pattern)\n",
        "    matches = matcher(nlp_doc)\n",
        "    for match_id, start, end in matches:\n",
        "        span = nlp_doc[start:end]\n",
        "        return span.text\n",
        "     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU78uwFHZp52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_full_name(nlp_doc):\n",
        "    nlp_doc = nlp(nlp_doc)\n",
        "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
        "    matcher.add('FULL_NAME', None, pattern)\n",
        "    matches = matcher(nlp_doc)\n",
        "    for match_id, start, end in matches:\n",
        "        span = nlp_doc[start:end]\n",
        "        return span.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgQUX4fjZp6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def noun_phrase(text):\n",
        "  \"\"\"extracts noun phrase from the text.\n",
        "    input\n",
        "      text: a pandas series\n",
        "    output\n",
        "      returns noun phrases as sets inside a list.\n",
        "  \"\"\"\n",
        "    arr = []\n",
        "    text = nlp(text)\n",
        "    for chunk in text.noun_chunks:\n",
        "        arr.append(chunk)\n",
        "    \n",
        "    return arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1C1pV-iZp6N",
        "colab_type": "code",
        "colab": {},
        "outputId": "d7122d0a-921f-4354-e7b3-42a10fdcbf47"
      },
      "source": [
        "x.apply(noun_phrase)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10                            [(доказанное, превосходство)]\n",
              "12                            [(доказанное, превосходство)]\n",
              "24                          [(remission, rates), (placebo)]\n",
              "25                                              [(message)]\n",
              "26         [(safety, profile, robust, trial, program, pts)]\n",
              "                                ...                        \n",
              "168524                                       [(verschiebe)]\n",
              "168525    [(besseres), (klinische, ansprechen, durch, ab...\n",
              "168526    [(besseres), (klinische, ansprechen, durch, ab...\n",
              "168527    [(besseres), (klinische, ansprechen, durch, ab...\n",
              "168528                                       [(adalimumab)]\n",
              "Name: content_message_local, Length: 113304, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEpf-K6RZp6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x.apply(extract_full_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJNV6rRMtL3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data2  = pd.read_csv(\"/content/drive/My Drive/Copy of data1.csv\")\n",
        "data2['content_message_local'] = data2['content_message_local'].apply(lambda x: str(x))\n",
        "data2.content_message_local =data2.content_message_local.replace('nan',np.NAN)\n",
        "data2.content_message_local = data2.content_message_local.astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8OX36Pmp1Ms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # # stemmer = SnowballStemmer('german')\n",
        "# x = list(data2['content_message_local'].dropna(axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv9CEkstp4mT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dictionary of german vocabulary\n",
        "dictionary = [\n",
        "      'Haus', \n",
        "      'besichtigung', \n",
        "      'vereinbarung', \n",
        "      'papier', \n",
        "      'drucken', \n",
        "      'maschiene',\n",
        "      'auto',\n",
        "      'bahn'\n",
        "    ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjatRhuNZp6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decompound(word, vocabulary):\n",
        "  '''\n",
        "    Decompounds a word.\n",
        "    This function tries to split a given word up into combinations\n",
        "    of different words from a given vocabulary.\n",
        "    word : pandas dataframe or series\n",
        "    vocabulary: list of different words that form a comound word\n",
        " \n",
        "  '''\n",
        "\n",
        "  # Prepare dictionary for O(1) lookup and stemming.\n",
        "  stemmer = SnowballStemmer('german')\n",
        "  dictionary = { w: w for w in set([ stemmer.stem(w) for w in vocabulary ]) }\n",
        "\n",
        "  # Dict containing found words within target word with their indexes\n",
        "  subwords = defaultdict(set)\n",
        "\n",
        "  # This list contains the indexes from which a word should be searched.\n",
        "\n",
        "  \n",
        "  todo = [0]\n",
        "\n",
        "  # Phase 1: Find all subwords\n",
        "  fugenlaute = ['e', 's', 'es', 'n', 'en', 'er', 'ens']\n",
        "\n",
        "  while todo:\n",
        "    if not todo[0] in subwords:\n",
        "      for i in range(todo[0] + 1, len(word) + 1):\n",
        "        current_subword = word[todo[0]:i]\n",
        "        current_subword_stemmed = stemmer.stem(current_subword)\n",
        "\n",
        "        if current_subword_stemmed in dictionary:\n",
        "          # determine possible indices for the \n",
        "          # next word.\n",
        "          \n",
        "          # 1. Just right after the non-stemmed subword\n",
        "          next_indices = [todo[0] + len(current_subword)]\n",
        "\n",
        "          # 2. If 'Fugenlaut' comes afterwards, respect that too\n",
        "          #    as a possible start of a new subword.\n",
        "          for j in range(1, 3):\n",
        "            tmp = next_indices[0] + j\n",
        "\n",
        "            if tmp > len(word):\n",
        "              break\n",
        "\n",
        "            if word[next_indices[0]:tmp] in fugenlaute:\n",
        "              next_indices.append(tmp) \n",
        "\n",
        "          # Add subword to dict \n",
        "          subwords[todo[0]] = subwords[todo[0]].union(next_indices)\n",
        "\n",
        "          # Update todo list\n",
        "          todo += next_indices\n",
        "\n",
        "    del todo[0]\n",
        "\n",
        "  # Phase 2: Try to combine all subwords\n",
        "  result = []\n",
        "\n",
        "  def tree_search(i, configuration):\n",
        "    if i == len(word):\n",
        "      if configuration:\n",
        "        # Found a match\n",
        "        result.append(configuration)\n",
        "\n",
        "      return\n",
        "\n",
        "    if not i in subwords:\n",
        "      return\n",
        "\n",
        "    for j in subwords[i]:\n",
        "      tree_search(j, configuration + [word[i:j]])\n",
        "\n",
        "  tree_search(0, [])\n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii9fBN4Z0Qjo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f568f4c2-c604-4e60-dab0-4564855357c2"
      },
      "source": [
        "data2.content_message_local[666]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Verschiebe die Grenzen'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lDk9mTV4vJq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "329d43af-695b-4928-880b-11fb43b26f08"
      },
      "source": [
        "data2.apply(lambda x: decompound(x.content_message_local, dictionary), axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         []\n",
              "1         []\n",
              "2         []\n",
              "3         []\n",
              "4         []\n",
              "          ..\n",
              "168528    []\n",
              "168529    []\n",
              "168530    []\n",
              "168531    []\n",
              "168532    []\n",
              "Length: 168533, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8_IEMHnsta4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bb7dfcac-2ecb-482d-da83-485fe786fa2c"
      },
      "source": [
        "decompound(data2.content_message_local[666],dictionary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPJe4dCXsuuo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d87f6d8-d6af-4c25-fb7d-22ca5538f1e2"
      },
      "source": [
        "# example\n",
        "decompound(\"Häuserbesichtigungsvereinbarungspapierdruckmaschienen\",dictionary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Häuser', 'besichtigungs', 'vereinbarungs', 'papier', 'druck', 'maschienen']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DFaLz0b1xQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Pkgs\n",
        "import spacy\n",
        "# Text Preprocessing Pkg\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation\n",
        "# Build a List of Stopwords\n",
        "stopwords = list(STOP_WORDS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpMjKOcSMZi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQLINXvcMeNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAL90S6hMgbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document1 = \"\"\"Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to progressively improve their performance on a specific task. Machine learning algorithms build a mathematical model of sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in the applications of email filtering, detection of network intruders, and computer vision, where it is infeasible to develop an algorithm of specific instructions for performing the task. Machine learning is closely related to computational statistics, which focuses on making predictions using computers. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a field of study within machine learning, and focuses on exploratory data analysis through unsupervised learning.In its application across business problems, machine learning is also referred to as predictive analytics.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKTyunEGMoAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document2 = \"\"\"Our Father who art in heaven, hallowed be thy name. Thy kingdom come. Thy will be done, on earth as it is in heaven. Give us this day our daily bread; and forgive us our trespasses, as we forgive those who trespass against us; and lead us not into temptation, but deliver us from evil\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsY8rVWUM1DA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNOcAXmjM293",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build an NLP Object\n",
        "docx = nlp(document1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMQQGwWIOBJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(doc):\n",
        "  # Tokenization of Text\n",
        "  doc = nlp(doc)\n",
        "  mytokens = [token.text for token in docx]\n",
        "  return mytokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSipSmPdOMSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_frequencies(doc):\n",
        "  # Build Word Frequency\n",
        "  # word.text is tokenization in spacy\n",
        "  doc = nlp(doc)\n",
        "  word_frequencies = {}\n",
        "  for word in docx:\n",
        "    if word.text not in stopwords:\n",
        "            if word.text not in word_frequencies.keys():\n",
        "                word_frequencies[word.text] = 1\n",
        "            else:\n",
        "                word_frequencies[word.text] += 1\n",
        "  # Maximum Word Frequency\n",
        "  maximum_frequency = max(word_frequencies.values())\n",
        "  for word in word_frequencies.keys():  \n",
        "        word_frequencies[word] = (word_frequencies[word]/maximum_frequency)\n",
        "  return word_frequencies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aiIcqu-M5Fh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_score(doc):\n",
        "  \n",
        "  # Sentence Tokens\n",
        "  sentence_list = [sentence for sentence in nlp(doc).sents]\n",
        "  # Sentence Score via comparing each word with sentence\n",
        "  sentence_scores = {} \n",
        "  for sent in sentence_list:  \n",
        "          for word in sent:\n",
        "              if word.text.lower() in word_frequencies(doc).keys():\n",
        "                  if len(sent.text.split(' ')) < 30:\n",
        "                      if sent not in sentence_scores.keys():\n",
        "                          sentence_scores[sent] = word_frequencies(doc)[word.text.lower()]\n",
        "                      else:\n",
        "                          sentence_scores[sent] += word_frequencies(doc)[word.text.lower()]\n",
        "  return sentence_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdWS6hpFM7Ka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def summarise_spacy(doc):\n",
        "  summarized_sentences = nlargest(7, sentence_score(doc), key=sentence_score(doc).get)\n",
        "  final_sentences = [ w.text for w in summarized_sentences ]\n",
        "  summary = ' '.join(final_sentences)\n",
        "  return summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "catx-Pf2M98w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def summarize_text(doc, word_count = 100):\n",
        "  \"\"\"\" \n",
        "      This function uses gensim summarize for text summary.\n",
        "      doc:  a pandas series or any text file.\n",
        "      word_count: number of words to be returned by the summary.\n",
        "  \"\"\"\n",
        "  summ_per = summarize(doc, word_count = word_count)\n",
        "  return summ_per"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83XxBLJANB_K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dff361dc-59c4-40fa-adc2-a0ba6e7da092"
      },
      "source": [
        "tokenize(document2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Machine',\n",
              " 'learning',\n",
              " '(',\n",
              " 'ML',\n",
              " ')',\n",
              " 'is',\n",
              " 'the',\n",
              " 'scientific',\n",
              " 'study',\n",
              " 'of',\n",
              " 'algorithms',\n",
              " 'and',\n",
              " 'statistical',\n",
              " 'models',\n",
              " 'that',\n",
              " 'computer',\n",
              " 'systems',\n",
              " 'use',\n",
              " 'to',\n",
              " 'progressively',\n",
              " 'improve',\n",
              " 'their',\n",
              " 'performance',\n",
              " 'on',\n",
              " 'a',\n",
              " 'specific',\n",
              " 'task',\n",
              " '.',\n",
              " 'Machine',\n",
              " 'learning',\n",
              " 'algorithms',\n",
              " 'build',\n",
              " 'a',\n",
              " 'mathematical',\n",
              " 'model',\n",
              " 'of',\n",
              " 'sample',\n",
              " 'data',\n",
              " ',',\n",
              " 'known',\n",
              " 'as',\n",
              " '\"',\n",
              " 'training',\n",
              " 'data',\n",
              " '\"',\n",
              " ',',\n",
              " 'in',\n",
              " 'order',\n",
              " 'to',\n",
              " 'make',\n",
              " 'predictions',\n",
              " 'or',\n",
              " 'decisions',\n",
              " 'without',\n",
              " 'being',\n",
              " 'explicitly',\n",
              " 'programmed',\n",
              " 'to',\n",
              " 'perform',\n",
              " 'the',\n",
              " 'task',\n",
              " '.',\n",
              " 'Machine',\n",
              " 'learning',\n",
              " 'algorithms',\n",
              " 'are',\n",
              " 'used',\n",
              " 'in',\n",
              " 'the',\n",
              " 'applications',\n",
              " 'of',\n",
              " 'email',\n",
              " 'filtering',\n",
              " ',',\n",
              " 'detection',\n",
              " 'of',\n",
              " 'network',\n",
              " 'intruders',\n",
              " ',',\n",
              " 'and',\n",
              " 'computer',\n",
              " 'vision',\n",
              " ',',\n",
              " 'where',\n",
              " 'it',\n",
              " 'is',\n",
              " 'infeasible',\n",
              " 'to',\n",
              " 'develop',\n",
              " 'an',\n",
              " 'algorithm',\n",
              " 'of',\n",
              " 'specific',\n",
              " 'instructions',\n",
              " 'for',\n",
              " 'performing',\n",
              " 'the',\n",
              " 'task',\n",
              " '.',\n",
              " 'Machine',\n",
              " 'learning',\n",
              " 'is',\n",
              " 'closely',\n",
              " 'related',\n",
              " 'to',\n",
              " 'computational',\n",
              " 'statistics',\n",
              " ',',\n",
              " 'which',\n",
              " 'focuses',\n",
              " 'on',\n",
              " 'making',\n",
              " 'predictions',\n",
              " 'using',\n",
              " 'computers',\n",
              " '.',\n",
              " 'The',\n",
              " 'study',\n",
              " 'of',\n",
              " 'mathematical',\n",
              " 'optimization',\n",
              " 'delivers',\n",
              " 'methods',\n",
              " ',',\n",
              " 'theory',\n",
              " 'and',\n",
              " 'application',\n",
              " 'domains',\n",
              " 'to',\n",
              " 'the',\n",
              " 'field',\n",
              " 'of',\n",
              " 'machine',\n",
              " 'learning',\n",
              " '.',\n",
              " 'Data',\n",
              " 'mining',\n",
              " 'is',\n",
              " 'a',\n",
              " 'field',\n",
              " 'of',\n",
              " 'study',\n",
              " 'within',\n",
              " 'machine',\n",
              " 'learning',\n",
              " ',',\n",
              " 'and',\n",
              " 'focuses',\n",
              " 'on',\n",
              " 'exploratory',\n",
              " 'data',\n",
              " 'analysis',\n",
              " 'through',\n",
              " 'unsupervised',\n",
              " 'learning',\n",
              " '.',\n",
              " 'In',\n",
              " 'its',\n",
              " 'application',\n",
              " 'across',\n",
              " 'business',\n",
              " 'problems',\n",
              " ',',\n",
              " 'machine',\n",
              " 'learning',\n",
              " 'is',\n",
              " 'also',\n",
              " 'referred',\n",
              " 'to',\n",
              " 'as',\n",
              " 'predictive',\n",
              " 'analytics',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73iFnHIjPKfq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "03b74086-4d3c-40f1-815b-eab321c518c3"
      },
      "source": [
        "word_frequencies(document2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\"': 0.2222222222222222,\n",
              " '(': 0.1111111111111111,\n",
              " ')': 0.1111111111111111,\n",
              " ',': 1.0,\n",
              " '.': 0.7777777777777778,\n",
              " 'Data': 0.1111111111111111,\n",
              " 'In': 0.1111111111111111,\n",
              " 'ML': 0.1111111111111111,\n",
              " 'Machine': 0.4444444444444444,\n",
              " 'The': 0.1111111111111111,\n",
              " 'algorithm': 0.1111111111111111,\n",
              " 'algorithms': 0.3333333333333333,\n",
              " 'analysis': 0.1111111111111111,\n",
              " 'analytics': 0.1111111111111111,\n",
              " 'application': 0.2222222222222222,\n",
              " 'applications': 0.1111111111111111,\n",
              " 'build': 0.1111111111111111,\n",
              " 'business': 0.1111111111111111,\n",
              " 'closely': 0.1111111111111111,\n",
              " 'computational': 0.1111111111111111,\n",
              " 'computer': 0.2222222222222222,\n",
              " 'computers': 0.1111111111111111,\n",
              " 'data': 0.3333333333333333,\n",
              " 'decisions': 0.1111111111111111,\n",
              " 'delivers': 0.1111111111111111,\n",
              " 'detection': 0.1111111111111111,\n",
              " 'develop': 0.1111111111111111,\n",
              " 'domains': 0.1111111111111111,\n",
              " 'email': 0.1111111111111111,\n",
              " 'explicitly': 0.1111111111111111,\n",
              " 'exploratory': 0.1111111111111111,\n",
              " 'field': 0.2222222222222222,\n",
              " 'filtering': 0.1111111111111111,\n",
              " 'focuses': 0.2222222222222222,\n",
              " 'improve': 0.1111111111111111,\n",
              " 'infeasible': 0.1111111111111111,\n",
              " 'instructions': 0.1111111111111111,\n",
              " 'intruders': 0.1111111111111111,\n",
              " 'known': 0.1111111111111111,\n",
              " 'learning': 0.8888888888888888,\n",
              " 'machine': 0.3333333333333333,\n",
              " 'making': 0.1111111111111111,\n",
              " 'mathematical': 0.2222222222222222,\n",
              " 'methods': 0.1111111111111111,\n",
              " 'mining': 0.1111111111111111,\n",
              " 'model': 0.1111111111111111,\n",
              " 'models': 0.1111111111111111,\n",
              " 'network': 0.1111111111111111,\n",
              " 'optimization': 0.1111111111111111,\n",
              " 'order': 0.1111111111111111,\n",
              " 'perform': 0.1111111111111111,\n",
              " 'performance': 0.1111111111111111,\n",
              " 'performing': 0.1111111111111111,\n",
              " 'predictions': 0.2222222222222222,\n",
              " 'predictive': 0.1111111111111111,\n",
              " 'problems': 0.1111111111111111,\n",
              " 'programmed': 0.1111111111111111,\n",
              " 'progressively': 0.1111111111111111,\n",
              " 'referred': 0.1111111111111111,\n",
              " 'related': 0.1111111111111111,\n",
              " 'sample': 0.1111111111111111,\n",
              " 'scientific': 0.1111111111111111,\n",
              " 'specific': 0.2222222222222222,\n",
              " 'statistical': 0.1111111111111111,\n",
              " 'statistics': 0.1111111111111111,\n",
              " 'study': 0.3333333333333333,\n",
              " 'systems': 0.1111111111111111,\n",
              " 'task': 0.3333333333333333,\n",
              " 'theory': 0.1111111111111111,\n",
              " 'training': 0.1111111111111111,\n",
              " 'unsupervised': 0.1111111111111111,\n",
              " 'use': 0.1111111111111111,\n",
              " 'vision': 0.1111111111111111}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VG6P96hYGSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document2 = \"Our Father who art in heaven, hallowed be thy name. Thy kingdom come. Thy will be done, on earth as it is in heaven. Give us this day our daily bread; and forgive us our trespasses, as we forgive those who trespass against us; and lead us not into temptation, but deliver us from evil\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8-D5HgJaYnl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "char_splitter = re.compile(\"[.,;!:()-]\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAF-3m4lO9Lq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_phrases(text, stopwords = nltk.corpus.stopwords.words('english')):\n",
        "    \"\"\" generate phrases using phrase boundary markers \n",
        "      text: pandas dataframe or series\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    # generate approximate phrases with punctation\n",
        "    coarse_candidates = char_splitter.split(text.lower())\n",
        "\n",
        "    candidate_phrases = []\n",
        "\n",
        "    for coarse_phrase\\\n",
        "            in coarse_candidates:\n",
        "\n",
        "        words = re.split(\"\\\\s+\", coarse_phrase)\n",
        "        previous_stop = False\n",
        "\n",
        "        # examine each word to determine if it is a phrase boundary marker or\n",
        "        # part of a phrase or lone ranger\n",
        "        for w in words:\n",
        "\n",
        "            if w in stopwords and not previous_stop:\n",
        "                # phrase boundary encountered, so put a hard indicator\n",
        "                candidate_phrases.append(\";\")\n",
        "                previous_stop = True\n",
        "            elif w not in stopwords and len(w) > 3:\n",
        "                # keep adding words to list until a phrase boundary is detected\n",
        "                candidate_phrases.append(w.strip())\n",
        "                previous_stop = False\n",
        "\n",
        "    # get a list of candidate phrases without boundary demarcation\n",
        "    phrases = re.split(\";+\", ' '.join(candidate_phrases))\n",
        "\n",
        "    return phrases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMl8lvvqXmDM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "fdd85eb8-ca75-42cc-f035-a21dbd4862db"
      },
      "source": [
        "generate_phrases(document2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " ' father ',\n",
              " ' heaven hallowed ',\n",
              " ' name kingdom come ',\n",
              " ' done ',\n",
              " ' earth ',\n",
              " ' heaven give ',\n",
              " ' daily bread ',\n",
              " ' forgive ',\n",
              " ' trespasses ',\n",
              " ' forgive ',\n",
              " ' trespass ',\n",
              " ' ',\n",
              " ' lead ',\n",
              " ' temptation ',\n",
              " ' deliver ',\n",
              " ' evil']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVVB9TkkXl50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mpoOpuBXlzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdnG4p7hXll8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9aepfrbWlSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ea529336-cefe-4f28-9bec-482ddf9690af"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8WwDOQuNM_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OkKwBkxNPzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duNPgeVbVtA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrbBEwKxNVIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lShc1zy2NZPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3cUQkR9NdJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gK4HLrANgWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na7C-8LpNjVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OeC1BfyNuF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUEU_o4VNybk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10j3XuEmN13Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqjTXgfvN9JB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}