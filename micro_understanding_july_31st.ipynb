{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "micro_understanding_july_31st.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNgmm2F0bUuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO \n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import spacy\n",
        "!pip install stanfordcorenlp\n",
        "!pip install neuralcoref\n",
        "# load english language model\n",
        "nlp = spacy.load('en_core_web_sm',disable=['ner','textcat'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r731VzuhVo3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "submission = \"https://drive.google.com/file/d/1Lu_TiHe4AIvv1Yng5PiRx7uKqHa0Lurh/view?usp=sharing\"\n",
        "\n",
        "\n",
        "# Creating a function to read a csv file shared via google\n",
        "\n",
        "def read_csv(url):\n",
        "  url = 'https://drive.google.com/uc?export=download&id=' + url.split('/')[-2]\n",
        "  csv_raw = requests.get(url).text\n",
        "  csv = StringIO(csv_raw)\n",
        "  df = pd.read_csv(csv,engine = \"python\")\n",
        "  return df\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUPx4sAaVonu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = read_csv(submission)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko__sTZF6IVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding=utf-8\n",
        "\n",
        "import json\n",
        "from stanfordcorenlp import StanfordCoreNLP\n",
        "def Coreference_resolution(text):\n",
        "\n",
        "    '''\n",
        "    text : pandas dataframe containg texts\n",
        "    '''\n",
        "    nlp = StanfordCoreNLP(r'G:\\JavaLibraries\\stanford-corenlp-full-2017-06-09', quiet=False)\n",
        "    props = {'annotators': 'coref', 'pipelineLanguage': 'en'}\n",
        "\n",
        "    result = json.loads(nlp.annotate(text, properties=props))\n",
        "\n",
        "    num, mentions = result['corefs'].items()[0]\n",
        "    for mention in mentions:\n",
        "        print(mention)\n",
        "    return mention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHQqAkD26IRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "import neuralcoref\n",
        "def Coreference_resolution1(doc):\n",
        "    '''\n",
        "      doc:pandas dataframe containg texts\n",
        "    '''\n",
        "\n",
        "    nlp = spacy.load('en')\n",
        "    neuralcoref.add_to_pipe(nlp)\n",
        "    for ent in doc.ents:\n",
        "        coef = ent._.coref_cluster\n",
        "\n",
        "    return coef"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo5EaRaD60EO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "2e096a95-9c47-4b7b-c323-54a63da4cc29"
      },
      "source": [
        "!pip install fuzzywuzzy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading https://files.pythonhosted.org/packages/43/ff/74f23998ad2f93b945c0309f825be92e04e0348e062026998b5eefef4c33/fuzzywuzzy-0.18.0-py2.py3-none-any.whl\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1HPqjna6H4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fuzzywuzzy import fuzz\n",
        "# Statistical â€“ similar to bottoms-up, but matches patterns against a statistically weighted database of patterns generated from tagged training data.\n",
        "def get_ratio(data1,data2,column1,column2):\n",
        "    '''\n",
        "    returns the ratio of the pattern being matched between the two datasets\n",
        "    data1 : pandas dataframe or series\n",
        "    data2 :  pandas dataframe or series\n",
        "    column1  : the name of the data1 column to search pattern in string format\n",
        "    column2  : the name of the dat2 column to match with data1\n",
        "    '''\n",
        "\n",
        "    name = data1[column1]\n",
        "    name1 = data2[column2]\n",
        "    return fuzz.token_set_ratio(name, name1)\n",
        "\n",
        "# data[data.apply(get_ratio, axis=1) > 70].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXKfE5ikCqwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_ratio1(data,column,database):\n",
        "    '''\n",
        "    returns the ratio of the pattern being matched between the two datasets\n",
        "    data : pandas dataframe or series\n",
        "    column  : the name of the data column to search pattern in string format\n",
        "    database : contains the patterns to check\n",
        "    '''\n",
        "\n",
        "    name = data[column]\n",
        "    name1  = database\n",
        "    return fuzz.token_set_ratio(name, name1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCwYzuW-6oTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h06IxxA0bAhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# top down pos\n",
        "def part_of_speech(text):\n",
        "    \"\"\"gives the part of speech the different segment in the text.\"\"\"\n",
        "    # POS tagging\n",
        "    #  \"nlp\" Objectis used to create documents with linguistic annotations.\n",
        "    docs = nlp(text)\n",
        "\n",
        "    for word in docs:\n",
        "        return word.text, word.pos_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-61kAkhhf-Zz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from spacy import displacy \n",
        "def  display(text):\n",
        "    '''\n",
        "    display the texts\n",
        "    text: pandas dataframe or series\n",
        "    '''\n",
        "    doc   = nlp(text)\n",
        "    return displacy.render(doc, style='dep',jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dievBdcHxmDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ie_preprocess(document):\n",
        "    '''\n",
        "    tokenize the text and returns the part of speech\n",
        "    documents: pandas dataframe or series containg the text\n",
        "\n",
        "    '''\n",
        "    sentences = nltk.sent_tokenize(document) \n",
        "    sentences = [nltk.word_tokenize(sent) for sent in sentences] \n",
        "    sentences = [nltk.pos_tag(sent) for sent in sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAwfnJJnxl3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px9vVa1IaUKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def verify_grammar(self):\n",
        "        \"\"\"\n",
        "        Return True if the grammar is a valid Probabilistic Context-Free Grammar in Chomsky normal form\n",
        "\n",
        "        Otherwise return False. \n",
        "        \"\"\"\n",
        "        # TODO, Part 1\n",
        "         \n",
        "        for key, value_array in self.lhs_to_rules.items(): \n",
        "            probs = [] \n",
        "            for value in value_array: \n",
        "                if not self.is_valid_left(key) or not self.is_valid_right(value[1]):\n",
        "                    return False;\n",
        "                probs.append(value[2]) \n",
        "\n",
        "            if abs(fsum(probs) - 1) > 0.000001: \n",
        "                return False \n",
        "\n",
        "        return True "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3gK4aR5aUDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ISnSE-uaT5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}